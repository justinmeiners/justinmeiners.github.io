<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
<title>Justin Meiners Site</title>
<description>Welcome to my personal site about programming, math, and philosophy!</description>
<link>https://justinmeiners.github.io/</link>
<lastBuildDate>Sun, 19 Sep 2021 16:11:52 </lastBuildDate>
<item>
<pubDate>Sun, 19 Sep 2021 00:00:00 </pubDate>
<title>Classic Colors</title>
<guid isPermaLink="true">https://github.com/justinmeiners/classic-colors</guid>
</item>
<item>
<pubDate>Sat, 31 Jul 2021 00:00:00 </pubDate>
<title>Efficient Programming with Components</title>
<guid isPermaLink="true">https://justinmeiners.github.io/efficient-programming-with-components/</guid>
</item>
<item>
<pubDate>Wed, 28 Jul 2021 00:00:00 </pubDate>
<title>Almost Solving the Halting Problem</title>
<guid isPermaLink="true">https://justinmeiners.github.io/almost-solving-the-halting-problem/halting-problem.pdf</guid>
</item>
<item>
<pubDate>Mon, 03 May 2021 00:00:00 </pubDate>
<title>Three Myths About Passive Investing</title>
<guid isPermaLink="true">https://justinmeiners.github.io/three-myths-passive-investing/</guid>
<description><![CDATA[ 
<h1>Three Myths about Passive Investing</h1>

<p><strong>5/3/2021</strong></p>

<p>Stock markets are at tremendous highs, not only in absolute price, but also in terms of <a href="https://www.multpl.com/s-p-500-earnings-yield">earnings yield</a>.
At the same time we see <a href="https://twitter.com/TikTokInvestors">unsophisticated participants</a> taking incredible risks in Robinhood, Gamestop, Dogecoin, and Tesla, indicating a public excitement alongside the financial metrics.
So is the top in?</p>

<p>Seasoned passive investor should be amused by this question.
It&rsquo;s merely commotion for financial news.
It matters for day traders and speculators.
But, by passively holding broad market index funds for a long time horizon, we don&rsquo;t need to worry about such things.
Someone will always insist &ldquo;this time is different&rdquo;, but
we just need to stay the course and ignore the market swings.
Right?</p>

<p>Broadly, this message is correct.
But, safe activities become dangerous when we become too comfortable.
Just as we see recklessness among
traders, a different brand of recklessness appears to be sneaking into
passive investing.</p>

<p>For example, in the market decline around March 2020, due to the spread of COVID-19 and the associated lock downs,
most Vanguard investors <a href="https://personal.vanguard.com/pdf/coronavirus-market-volatility.pdf">purchased additional equities</a> rather than selling
or acquiring less volatile assets.
Buying when others are fearful is the adage,
but if everyone is buying, the consensus is actually optimistic.
Either:</p>

<ol>
<li>Investors didn&rsquo;t really believe economic effects would be that bad.</li>
<li>Investors discounted the negative effects years into the future and rebalanced out of cash.</li>
<li>Investors believed stocks would go up, regardless of negative economic effects.</li>
</ol>


<p>Even among skeptics 1. wasn&rsquo;t common during the first few weeks of uncertainty.
Many investors held belief 2., but considering savings rates and that younger people need capital in coming years (such as for down payments on homes) its hard to believe the majority are prepared for their portfolios to be below par for 5 to 10 years.</p>

<p>This is concerning because those in camp 3 believe they can&rsquo;t lose.
No matter what happens they will make money in a time frame usable for them.
This isn&rsquo;t what passive investing teaches, and is instead
based on passive myths.</p>

<h2>Myth #1: Asset prices don&rsquo;t matter.</h2>

<p>New traders learn that &ldquo;buy high&rdquo; &ldquo;sell low&rdquo; is a lot harder than it sounds.
It&rsquo;s not just hard to find opprotunities, psychologically people seem biased towards losing money.
Passive investors observe teach &ldquo;time in the market&rdquo; tends to beat &ldquo;timing the market&rdquo;.
Instead, the passive investor focuses on accumulation of assets over the long term rather than trading price trends.</p>

<p>This is sound investment advice, but many investors
have concluded that this means price doesn&rsquo;t matter.
We should pay any price, because no matter what the index will go up and we will eventually get our returns.
(If you believe this, can I sell you some stock?)</p>

<p>Let&rsquo;s make an axiomatic observation.
The gross yield of an investment is:</p>

<pre><code>(price sold + dividends recieved) / price purchased
</code></pre>

<p>(Inflation and time discounting proportional are proportional modifications).
Thus, increasing purchase price, lowers returns.
Therefore, <strong>price purchased determines your returns</strong>,
so it cannot be ignored.
Consequently we should always try to pay the lowest prices possible.</p>

<p>So is it time to start reading charts? No.
Markets tend to do well at pricing assets (especially on public exchanges), and its difficult to  predict where price will go.
Purchasing at regular intervals (of any duration) at market prices, such as dollar-cost averaging,
is an effective strategy to pay fair prices, and avoid extremes which may occur for short periods.</p>

<p><a href="https://www.bogleheads.org/wiki/Rebalancing">Rebalancing</a> between assets at regular periods is another way to &ldquo;buy high&rdquo; and &ldquo;sell low&rdquo;.
Adjusting asset allocation in response to extreme price moves is not day trading.</p>

<h3>Revision: Always get the best prices you can and don&rsquo;t overpay. Dollar-cost averaging is a strategy for getting good prices.</h3>

<h2>Myth #2: Buying broad market indices is the optimal investment strategy.</h2>

<p>Passive investing is based on the idea that large markets typically price assets efficiently.
<a href="https://www.investopedia.com/terms/m/modernportfoliotheory.asp">Modern portfolio theory</a> attempts to describe how to earn returns in such an environment,
and is supportive of passive investing.
It argues that assets are primarily distinguished by their level of risk and time horizons.
Long term risky investments are cheaper than short term sure bets.
Since the market understands how to price levels of risk, there is no free lunch.</p>

<p>Picture each ETF or mutual fund (such as an S&amp;P index) lying on a graph of risk to return.
Given this model, its absurd to think any one investment gives the greatest possible returns.
Unless its the riskiest asset available, one can always shift down the risk curve
and find something with higher returns.
If S&amp;P is a sure thing, wouldn&rsquo;t leveraged S&amp;P be even better?</p>

<p>The real question is how an index performs <a href="https://en.wikipedia.org/wiki/Efficient_frontier"><em>relative</em></a> to other assets
with the same level of risk. The effectiveness of popular indices like S&amp;P as benchmarks
for other assets, suggests it performs well.
But, you it would still be hard to argue that it is the <em>best</em> for all time.</p>

<p>An enormous advantage to buying an index is that is easy and cheap.
That&rsquo;s worth a lot, even if a complex combination exists that is slightly better.</p>

<h3>Revision: Passive investing can offer good returns relative to risk levels and is easy.</h3>

<h2>Myth #3: It&rsquo;s impossible for an investor or fund to beat broad market indices.</h2>

<p>When a <a href="https://www.bogleheads.org/">Boglehead</a> is shown a fund that outperforms a popular index, they immediately
conclude it must be fraudulent, or soon headed for disaster.
We just learned that one possible explanation is that they are simply taking on more risk.
But another is that the fund is run by people with expertise, putting in time
to squeeze out extra risk/return efficiency..
After all, we believe markets are efficient because there are experts of varying beliefs
 scrutinizing all kinds of information.</p>

<p>Let&rsquo;s imagine five of your friends are indepdendtly starting restaurants and seek your investment.
You are going to invest in a few of them.
They offer to present their finances and business plan to you.
Do you think it would be worth consider that information, or would you be better
off just picking them randomly?</p>

<p>You can probably think of some obvious information that would be much better than random.
Have any of them run a business before? Were they successful? How much money do they have?
Is anyone else helping them?
It may turn out your picks are unlucky, or others are successful underdogs,
but its obvious at the small scale that its possible for you to make some judgment about
their investment quality.</p>

<p>Professionals can ask the same kinds of questions about large companies.
They can study business and corporate leadership, accounting practices, and
perhaps having run businesses of their own.
So they should be able to identify good businesses from bad, or at least eliminate the worst contenders.</p>

<p>This is made complicated by the context of a global economy, with so much uncertainty,
and interdependence. But, the principle is no different.
<strong>It&rsquo;s possible for a person to reason about whether a business will succeed or not.</strong>
If you accept that principle, then you can expect some professionals might
make investments better than random, even if it takes a lot of effort and skill
to obtain much of an edge,</p>

<p>Whether these kinds of people work at any of the high-fee funds in your company 401k is a
separate question.
As a matter of practicality, it may not be worth looking for them.
But good investors almost certainly exist.
There <em>will</em> be funds that outperform passive funds.
That is entirely OK.
The idea of passive investing is that its easy and cheap.
You can beat <em>most</em> investors just by purchasing a broad market index.
Isn&rsquo;t that fantastic?</p>

<h3>Revision: Professional investors participate in pricing assets. They can often make additional money through this activity and expertise. Passive funds are easier and cheaper than most publicly available investment products.</h3>

<p>One commonality among these myths is that they imply you no longer have to think.
Prices are always correct. Stop thinking about whether you&rsquo;re over paying and buy.
Stop thinking about whether an investment is worthwhile or if you should consider, better alternatives.
The market is just unknowable, so you might as well not try.
That doesn&rsquo;t sound anything like shrewd investing,
it sounds like marketing.</p>

<p>If we accept the revised statements there is endless opportunity for learning and improvement.
What are good strategies for purchasing and rebalancing?
Are there other funds I should consider in my portfolio?
Are there investment opportunities in my local community?
All of these questions are worth considering slowly and methodically.</p>

 ]]></description>
</item>
<item>
<pubDate>Tue, 06 Apr 2021 00:00:00 </pubDate>
<title>Master&#x27;s Thesis: Computing the Rank of Braids</title>
<guid isPermaLink="true">https://github.com/justinmeiners/braid-rank-thesis</guid>
</item>
<item>
<pubDate>Sun, 27 Dec 2020 00:00:00 </pubDate>
<title>Advent of Code 2020 Day 18: Rethinking Operator Precedence</title>
<guid isPermaLink="true">https://justinmeiners.github.io/aoc-2020-18</guid>
<description><![CDATA[ 
<h1>Advent of Code 2020 Day 18: Rethinking Operator Precedence</h1>

<p><strong>12/27/20</strong></p>

<p>Part 2 of  <a href="https://adventofcode.com/2020/day/18">AOC Day 18</a> asks
you to evaluate some math expressions, but with a twist.
<code>+</code> has a greater operator precedence than <code>*</code>.
(This is opposite of how we typically read math):</p>

<pre><code>2 * 3 + (4 * 5)
2 * 3 + 20
2 * 23
46
</code></pre>

<p>The standard method for expression evaluation is the <a href="https://en.wikipedia.org/wiki/Shunting-yard_algorithm">shunting yard</a> algorithm.
This is the <em>right</em> and efficient way to solve this problem.
But, it&rsquo;s also not very obvious or natural.
It&rsquo;s hard to imagine how you would think if it yourself,
and doesn&rsquo;t really match how people actually see mathematical expressions.</p>

<p>I want to show off an alternative solution demonstrating
how you could think of it yourself.
My code is written in Common Lisp.
For this particular problem this choice goes a bit deeper than just syntax
as the solution makes heavy use of a symbolic expression manipulation which is harder to do in most languages.</p>

<p><strong>Thinking about Precedence</strong></p>

<p>What does operator precedence actually mean?
It tells us the <em>order in which to evaluate operators</em>.
So naively we should just do all the additions first, and then the multiplications.
Can we program that?
We certainly can, but doing just one operation at a time,
means at each step we produce an intermediate result which is itself a math expression, just simplified a bit.
For example given the expression:</p>

<pre><code> (1 + 2 * 3 + 4)
</code></pre>

<p>We first evaluate all the <code>+</code> to produce:</p>

<pre><code>(3 * 7)
</code></pre>

<p>And then we evaluate all the &lsquo;*&rsquo; to produce:</p>

<pre><code>(21)
</code></pre>

<p>After evaluating every operator, we should get a list containing a single number.</p>

<p>Below is some code which does just that.
The input <code>chain</code> is an expression (list of numbers and operators)
and <code>allowed</code> is the operator we want to evaluate.</p>

<pre><code>(defun simplify-chain (chain allowed)
  (prog ((result '())
         (op nil)
         (l nil)
         (r nil))

        (setf l (car chain))
        (setf chain (cdr chain))

        loop
        (setf op (car chain))
        (setf chain (cdr chain))

        (setf r (car chain))
        (setf chain (cdr chain))

        (if (eq op allowed)

            ; evaluate the operation.
            ; put the result back in the expression

            (setf l (funcall op l r))

            ; not an operation we want to evaluate right now.
            ; put it's symbols back in an the expression
            (progn (push l result)
                   (push op result)
                   (setf l r)))

        (if (consp chain)
            (go loop))

        (push l result)
        (return (reverse result))))
</code></pre>

<p><strong>Handling Parenthesis</strong></p>

<p>That is quite straightforward!
All the trickiness is fiddling with infix.
But, there is one problem.
Due to parenthesis the operands may not be &ldquo;ready&rdquo; to evaluate.
If we have:</p>

<pre><code>3 + (2 * 4)
</code></pre>

<p>We can&rsquo;t apply the <code>+</code> operation until the <code>(2 * 4)</code> is taken care of.
But evaluating <code>(2 * 4)</code> is something we could do directly.
There are only finitely many nested
statements, so eventually following parenthesis must end with a list containing only numbers and operators.
This is clearly a recursive problem!
We need to evaluate all the parenthesized statements in a list, before we can evaluate
the list itself.</p>

<p>The algorithm for evaluation looks like the following:</p>

<ul>
<li>To evaluate a number or operator symbol, just return it. (base case)</li>
<li><p>To evaluate a list:</p>

<ol>
<li>evaluate every entry in the list. (take care of parenthesis)</li>
<li>simplify the expression by applying each operator in order. (simplify chain)</li>
<li>Return the only item left in the list.</li>
</ol>
</li>
</ul>


<p>Let&rsquo;s write an eval function which does this, building upon simplify chain:</p>

<pre><code>(defun eval-expr (expr)
  (if (listp expr)
        (car (reduce #'simplify-chain 
                     '(+ *)
                     :initial-value (mapcar #'eval-expr expr)))
        expr))
</code></pre>

<p>To summarize <code>simplify-chain</code> takes mathematical expressions
and returns expressions that are simpler, while <code>eval-expr</code> always returns a number.
Note that this works for any number of operators and adjusting
precedence is as simple as changing the order of the <code>'(+ *)</code> list.</p>

 ]]></description>
</item>
<item>
<pubDate>Sat, 19 Dec 2020 00:00:00 </pubDate>
<title>Advent of Code 2020 Day 19: An Easy way to do Part 2</title>
<guid isPermaLink="true">https://justinmeiners.github.io/aoc-2020-19</guid>
<description><![CDATA[ 
<h1>Advent of Code 2020 Day 19: An Easy way to do Part 2</h1>

<p><strong>12/19/20</strong></p>

<p>A fun mechanic in <a href="https://adventofcode.com/2020">AOC</a> is that the problem is divided
into two parts, with the second only revealed after completion of
the first.
This challenges you to adapt an initial solution to new requirements,
encouraging flexibility of design.
In the case of <a href="https://adventofcode.com/2020/day/19">problem 19</a>, a seemingly minor change, completely transforms
the requirements.
It may seem you have to rethink everything from scratch,
but there is actually a tiny change to part 1 which solves the problem</p>

<p>My personal ranking wasn&rsquo;t great, but you can see I still
managed to catch up a bit on part 2 (even taking an additional hour!) as others got stuck.</p>

<p><img src="personal_stats.png" alt="personal stats aoc 2020" /></p>

<p><strong>SPOILERS AHEAD</strong></p>

<h2>Part 1</h2>

<p>Let&rsquo;s outline a fairly standard solution to part 1.
We will stick to psuedocode, but at then end I will show some of my Common Lisp code.</p>

<p><strong>Parsing</strong></p>

<p>We need to parse the rules and turn them into a structure we can work with.
I chose to transform them into an expression tree.
A rule such as:</p>

<pre><code>3 4 | "a" "b"
</code></pre>

<p>Becomes:</p>

<pre><code>(or (3 4) ("a" "b"))
</code></pre>

<p>All the rules are stored in an array,
with their index corresponding to their rule number.</p>

<p>Lisp makes it easy to parse.
I first replace the <code>|</code> with <code>^</code> because pipe has special semantics for the lisp reader.
Whenever a rule contains a <code>^</code> I split it&rsquo;s components
into a left side and right side and wrap them in an <code>or</code>.</p>

<p><strong>Evaluation</strong></p>

<p>Now we need a recursive evaluation function to handle the rule.
It takes a rule and a string as input.
It returns <code>FAIL</code> if it cannot match,
or the modified string location, if it did match.
For example when the rule &ldquo;a&rdquo; is applied to the following string:</p>

<pre><code>"abc"
</code></pre>

<p>It returns &ldquo;bc&rdquo; indicating successful matching.</p>

<p>The evaluation recursively handles the following cases:</p>

<ul>
<li>evaluate a char: check that the first character agrees</li>
<li>evaluate a number: lookup the rule at that index and evaluate it.</li>
<li>evaluate an or form <code>(OR r1 r2)</code>: evaluate both options. If one succeeded, return that. Otherwise fail.</li>
<li>evaluate a list <code>(r1, ... ,rn)</code>: evaluate each rule in the list in order. Stop early if there is a failure.
                 If the string ends before the list ends, the match failed.</li>
</ul>


<p>Success is indicated by returning an empty string.
This means we used everything up and it matched.
Test how many entries pass and you&rsquo;re done.</p>

<h2>Part 2</h2>

<p><strong>What is going on?</strong></p>

<p>Part 2 makes two substitutions to the rules list.</p>

<pre><code>8: 42        -&gt;    8: 42 | 42 8
11: 42 31    -&gt;    11: 42 31 | 42 11 31
</code></pre>

<p>It&rsquo;s not immediately clear why this breaks our recursive evaluation.
In fact, if you run it, it will not crash and still give you an (incorrect) answer.
So what is going on? (This question took most of my time!)</p>

<p>Consider the following rule:</p>

<pre><code>"a" | "a" "b"
</code></pre>

<p>The string &ldquo;ab&rdquo; now matches <em>BOTH</em> possible choices. How do we know which one to pick?
Logically, a disjunction (or) operation shouldn&rsquo;t care.
But whatever choice
is made will actually have an effect on whether the remaining rules in the pattern
match.</p>

<p>Here is an example of what can go wrong.
Suppose our string is &ldquo;abc&rdquo;.</p>

<pre><code>0: 1 2
1: "a" | "a" "b"
2: "c"
</code></pre>

<p>Starting with rule 0, we try rule 1.
<code>"abc"</code> matches both <code>"a"</code> and <code>"a" "b"</code>.
So let&rsquo;s pick the first rule.
Now onto rule &ldquo;c&rdquo; which does not match &ldquo;b c&rdquo;.
The match fails.</p>

<p>If we had chosen &ldquo;a&rdquo; &ldquo;b&rdquo; then we move
to rule 2 &ldquo;c&rdquo; matches the remainder &ldquo;c&rdquo;.
We have a successful match.</p>

<p><strong>The Hard Solution</strong></p>

<p>Every time we an encounter an <code>OR</code> and both cases
match, we no longer know which one to pick.
The global success of a match can no longer be determined locally.</p>

<p>We aren&rsquo;t going to get false positives, but we may get false negatives
if we choose the wrong path to take.</p>

<p>A proper solution must diverge at every or, creating branching possible evaluations.
This can certainly be implemented, but is going to look very different than part 1,
and be more complicated.</p>

<p><strong>An Easy Solution</strong></p>

<p>What we really want to know is whether there is <em>SOME</em> set of choices for each OR that will cause the pattern to match.
Since we don&rsquo;t know what they are, we can just try a whole bunch, and see if we can find a set that does.
The easiest way to do this is by introducing probability.
Randomly pick a branch at each or.
We can then run the matcher a bunch of times and see if it finds a set of choices that match.</p>

<p>Let&rsquo;s work through the details.
We want to replace the code for &ldquo;evaluate an or form <code>(OR R1 R2)</code>&rdquo;.
The original descrpition was:</p>

<ul>
<li>If one succeeded, return that. Otherwise fail.</li>
</ul>


<p>Let&rsquo;s change it to a randomized choice when we don&rsquo;t have a better option:</p>

<ul>
<li>evaluate both options. If either one failed, return the other.
  Otherwise both succeeded, so randomly pick which one to return.</li>
</ul>


<p>Now for each string run the pattern matcher a few hundred times.
Each time it will (hopefully) try different choices for each OR.
With enough tries, it should get lucky and find the best path
if there is one.</p>

<p>Remember, changing branch directions never introduces false positives, only false negatives.
So if it matches even one time, then we know it works.
Experiment with different numbers of times until you get a match rate that stabilizes.
If you don&rsquo;t run it enough times, then you will get changing answers each time you run it.
It works!</p>

<p>Want to learn more? Checkout <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-28.html#%_sec_4.3">chapter 4.3</a> of SICP
on non-deterministic computing.</p>

<h2>My Code</h2>

<p>Here are the key snippets,
some of it is a little ugly.</p>

<p><strong>Eval (With Random)</strong></p>

<pre><code>(defun eval-rule (str rule all-rules)
  (cond ((stringp rule) (if (char= (car str) (char rule 0))
                            (cdr str)
                            'FAIL))

        ((numberp rule) (eval-rule str (aref all-rules rule) all-rules))
        ((and (listp rule) (eq 'or (car rule))) 
         (let ((first (eval-rule str (nth 1 rule) all-rules))
               (second (eval-rule str (nth 2 rule) all-rules)))
           (if (eq 'FAIL first)
               second
               (if (eq 'FAIL second)
                   first
                   (if (= (random 2) 1) 
                       first
                       second
                       )))))
        ((listp rule)
         (prog ((s str)
                (l rule)
                (result nil))

               step
               (setf result (eval-rule s (car l) all-rules))

               (if (eq result 'FAIL)
                   (return result))

               (setf s result)
               (setf l (cdr l))

               (if (consp l)
                   (if (consp s)
                        (go step)
                        (return 'FAIL))
                   (return s))
              ))
        (t (error "unknown rule "))))
</code></pre>

<p><strong>Solve Part 2</strong></p>

<pre><code>(defun solve-2 (all-rules inputs)
  (remove-if-not 
    (lambda (string)
      ; use probablity!
      (some #'identity (loop for i from 1 to 500 collect
                             (not (eval-rule (coerce string 'list) (aref all-rules 0) all-rules)))))
      inputs))
</code></pre>

 ]]></description>
</item>
<item>
<pubDate>Sun, 15 Nov 2020 00:00:00 </pubDate>
<title>The Universal Property of Quotients</title>
<guid isPermaLink="true">https://justinmeiners.github.io/universal-property-quotients/</guid>
<description><![CDATA[ 
<h1>The Universal Property of Quotients</h1>

<p><strong>11/15/20</strong></p>

<p><img src="1.png" alt="universal property of quotients" /></p>

<p>The first time you encounter a theorem concluding with &ldquo;an arrow that
makes the diagram commute&rdquo; can be quite confusing.
But with a little thought you can typically find that the idea expressed the theorem is obvious.
The provided arrow is simply the one thing that could possibly fit.
It may be tedious to construct, but understanding the theorem clarifies
why it must always be there, revealing patterns hidden in the construction.</p>

<p>Let&rsquo;s see how this works by studying the universal property of quotients, which was
the first example of a commutative diagram I encountered.
If you are familiar with topology, this property
applies to quotient maps.
But we will focus on quotients induced by equivalence relation on sets and ignored
additional structure.</p>

<p>An equivalence relation <code>~</code> is a binary relation satisfying the following properties:</p>

<ul>
<li>reflexive: <code>x ~ x</code></li>
<li>symmetric: <code>x ~ y &lt;=&gt; y ~ z</code></li>
<li>transitive:  <code>x ~ y and y ~ z implies x ~ z</code>.</li>
</ul>


<p>Examples include equality of real numbers, whether numbers are both even or odd (parity),
matrix similarity, isomorphism, etc.</p>

<p><strong>Exercise:</strong> Prove matrix similarity is an equivalence relation.</p>

<p>Given an equivalence relation ~ and an element <code>x</code> we can form it&rsquo;s equivalence
class <code>[x]</code> which is the set of things equivalent to it.
The set of a equivalence classes form a new set <code>X/~</code> with an analogous
structure to the original, but with portions &ldquo;grouped up&rdquo; or &ldquo;collapsed&rdquo;</p>

<p><strong>Exercise:</strong> Prove distinct equivalence classes are disjoint.</p>

<h2>Universal Property of Quotients</h2>

<p>Let <code>q(x) = [x]</code> be a map from an element to it&rsquo;s equivalence class.
Given a map <code>f</code> from <code>X</code> to <code>Y</code> which is constant on equivalence
classes (<code>q(x) = q(y) =&gt; f(x) = f(y)</code>),
we obtain a unique map from <code>X/~</code> to <code>Y</code> making the diagram commute.</p>

<p><img src="1.png" alt="universal property of quotients" /></p>

<p>What is going on here? <code>f</code> isn&rsquo;t just any map,
it&rsquo;s constant on equivalent elements of <code>X</code>.
So a lot of the information in <code>X</code> isn&rsquo;t really needed to compute the image of <code>f</code>.
Since equal elements get sent to the same place,
we could imagine picking just one element from each class and seeing where it goes.</p>

<p><img src="2.png" alt="equivalence collapse" /></p>

<p>In other words, we could define a function on each equivalence class.
This map would have the same image as <code>f</code> and this is precisely what the universal property tells us
is possible</p>

<p>Let&rsquo;s apply this theorem to a particular example and attempt to fill in the diagram.
Suppose our domain is the solid disc.</p>

<p><img src="3.png" alt="disc" /></p>

<p>To construct an equivalence relation on the disk, think of  properties that make points in the disc similar to one another.
One such property is their distance from the center.
Define <code>x ~ y</code> whenever <code>||x|| ~ ||y||</code>.
What does the quotient space <code>X/~</code> look like under this relation?
Each class is a ring at a particular radius <code>L</code>, so denote it <code>[L]</code>.
The radii of course vary continuously so we get set of classes isomorphic
to a closed interval <code>[0, 1]</code>.</p>

<p><img src="4.png" alt="disc radii classes" /></p>

<p><strong>Exercise:</strong> Find an equivalence relation on D<sup>2</sup> without 0 (punctured disc) whose classes
form the circle.</p>

<p>Now define a function <code>f(x) = x^2 + y^2</code> so our codomain is the real numbers R.
The graph is a multivariable calculus style paraboloid living in <code>R^3</code> above
the unit disc:</p>

<p><img src="5.png" alt="parabaloid" /></p>

<p>Can we apply the universal property?
Almost, we need to confirm <code>f</code> is constant on equivalence classes.
Suppose <code>(x, y) ~ (a, b)</code>.
Then <code>sqrt(x^2 + y^2) = sqrt(a^2 + b^2)</code>.
Squaring both sides we get that <code>f(x, y) = f(a, b)</code>.</p>

<p>Ah! We can clearly see <code>f</code> only depends on radius.
No matter what angle you are at <code>f</code> does the same thing.
So the disc is more information than we need,
we can define a similar function on the space of equivalence classes <code>X/~ = [0, 1]</code>.
Can you figure out what it is?</p>

<p>&hellip;</p>

<p>The missing function is of course <code>h([L]) = L^2</code>.
It&rsquo;s graph is a parabola in 2D which carves out the same range as <code>f</code> in the real numbers.</p>

<p><img src="6.png" alt="graph parabola" /></p>

<p>To check commutativity take a point <code>(x, y)</code> and apply <code>f(x, y) = x^2 + y^2</code>.
Now send it to it&rsquo;s equivalence class <code>q(x, y) = [L]</code> where <code>L = sqrt(x^2 + y^2)</code>.
Now <code>h(L) = x^2 + y^2 = f(x, y)</code>.</p>

<p>Easy right? You probably could see right away that <code>f</code> was just the square of the distance.
In other examples constructing such a function <code>h</code> might be less obvious,
but the universal property tells us it is always there.</p>

<p>Next time you an encounter a commutative diagram proof, try a few examples
to figure out what basic idea it is telling you.</p>

<p><strong>Exercise:</strong> Prove that <code>h</code> is unique. No other function could make the diagram commute.</p>

<h2>Further Reading</h2>

<p>Commutative diagrams are the central focus of category theory which attempts to understand
such properties at a higher level of abstraction than set theory.
From a category theory perspective the quotient set <code>X/~</code> is the co-equalizer
or co-limit of the diagram projecting and equivalent pair to it&rsquo;s parts.</p>

<p>You can read more about category theory from Topoi by Robert Goldblatt.</p>

<p><a href="https://www.amazon.com/gp/product/B00DP7UMC6/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B00DP7UMC6&amp;linkCode=as2&amp;tag=jmeiners-20&amp;linkId=c7f03020c8207f7d6ba3dc63bb0be29f"><img src="topoi.jpg" alt="Topoi Goldblatt" /></a></p>

<p>Free groups provide another elementary example of universal properties.
You can read about them in chapter 6.3 of Abstract Algebra by Dummit and Foote.</p>

<p><a href="https://www.amazon.com/gp/product/0471433349/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0471433349&amp;linkCode=as2&amp;tag=jmeiners-20&amp;linkId=4295d44052521c73c93e090190c032f0"><img src="dummit_and_foote.jpg" alt="Dummit and Foote" /></a></p>

<p>Quotient spaces are studied in depth in Topology by Munkres.</p>

<p><a href="https://www.amazon.com/gp/product/0134689518/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0134689518&amp;linkCode=as2&amp;tag=jmeiners-20&amp;linkId=a1ca862d48cd664489c7179c1f7bfdaf"><img src="munkres.jpg" alt="Munkres Topology" /></a></p>

<p> The later chapter on Algebraic Topology have
More elaborate constructions.
These gave me a lot of practice with commutative diagrams,
such as the chapter on <a href="https://en.wikipedia.org/wiki/Seifert%E2%80%93van_Kampen_theorem">Seifert-Van Kampen&rsquo;s</a> theorem.</p>

<p><img src="van-kampen.png" alt="van-kampen" /></p>

 ]]></description>
</item>
<item>
<pubDate>Thu, 08 Oct 2020 00:00:00 </pubDate>
<title>Questioning Probablity</title>
<guid isPermaLink="true">https://justinmeiners.github.io/questioning-probability/</guid>
<description><![CDATA[ 
<h1>Questioning Probability</h1>

<p><strong>10/8/20</strong></p>

<h3>Are Elections Actually Random?</h3>

<p>A popular narrative explaining the outcome of the 2016 election is that the odds were in Clinton&rsquo;s favor.
She just got unlucky.
That election night, dice were cast, and she unfortunately rolled snake eyes.
The comforting messages implied by this interpretation probably explains it&rsquo;s popularity;
it was nobody&rsquo;s fault,
we still have the popular ideas,
we don&rsquo;t even have to change our strategy to win next time.
The alternatives are much harder to look at.
But, the critical, yet unquestioned assumption that this narrative
depends on is that elections are random events.
But how similar is winning election to winning roulette?</p>

<p>Let&rsquo;s briefly relate what actually happens on voting day.
People wake up in the morning and make a decision whether to make their way to vote or not.
They physically walk or drive to a voting location, and then choose who&rsquo;s name to record, either by pushing a button or writing it in.
Later, the votes are tallied and grouped by region and a winner is announced.</p>

<p>Where is the randomness?
It&rsquo;s hard to even find where variables might be changing rapidly.
The media framework and campaign context is a product of months of effort.
Each individuals broad political views are slow moving.
They are driven by countless deeply rooted influences such as family upbringing, income, ethnicity, religion, education, culture.
Undoubtedly all of this is very complex, much if it is unconscious and affected by PR and marketing,
 but nowhere do we see anything that looks like a random occurrence.
It&rsquo;s always cause and effect, and it&rsquo;s not all that mysterious as you can probably make a good guess of how
an acquaintance will vote.</p>

<p>You might argue this isn&rsquo;t what is really meant when &ldquo;chance&rdquo; is talked about in elections.
Journalists aren&rsquo;t implying they function like a lottery.
The percentages they throw around are confidence intervals.
Pollsters can&rsquo;t talk to everyone in the country, so they talk to a
sample, and then use probability to guess how likely it is that
this sample represents the whole population.
It&rsquo;s basic statistics.</p>

<p>But, even if the numbers come from such methods,
this is not how they are presented by journalists and
otherwise intelligent and &ldquo;scientific&rdquo; people don&rsquo;t understand them that way either.
If so, they would be much more concerned about finding specific gaps between the sample and population
instead of wondering what the odds look like on their bet.
Just look at how prevalent the random model is in these examples:</p>

<p><strong>Election as a Pachinko Machine (1)</strong></p>

<p><img src="pachinko.png" alt="pachinko machine" /></p>

<p>Let&rsquo;s hope the people making these promotions
aren&rsquo;t the same as those making the models.</p>

<p><strong>Election as an Unlucky Dice Roll</strong></p>

<p><img src="random_roll.png" alt="random roll" /></p>

<p>Unlucky again?</p>

<p><strong>A Small Probability is still a Possibility</strong></p>

<p><img src="math.png" alt="math" /></p>

<p>Perhaps students need fewer stats classes and more critical thinking.</p>

<h3>Our Cultural of Randomness</h3>

<p>Questionable applications of random thinking aren&rsquo;t confined to politics.
This reflects an underlying cultural trust and belief in the power of statistics.
At first glance it appear to fit with our love of science and intelligence.
Just look at all the numbers, equations, and graph.
Everyone knows smart people use data to make decisions and
it overcomes biases.
Didn&rsquo;t you watch Money Ball?</p>

<p>But this belief actually begins with our our basic physical view;
the world is atoms crashing into each other in the void,
and at the atomic level quantum events are happening which we don&rsquo;t understand.
Fundamentally, the  future is indefinite, uncertain, and hazy.
We can&rsquo;t really plan or predict it.
Sometimes we can setup Fleming or Darwinian-like projects to tip chance
in our favor,
but the best thing to do is leave all your options open to respond to unforeseen conditions.
This indefinite attitude permeates our world with &ldquo;lean startups&rdquo;,
index funds, zero interest, zero savings, extracurricular, and no planning. (2)</p>

<p>But let&rsquo;s examine this views underlying underlying premise.
How many things in our world are actually random?
If you believe basic physics, it doesn&rsquo;t appear anything at the scale of everyday life is really random.
Every physical action is driven by cause and effect, which can be described
at the human scale can be accurately using deterministic Newtonian relationships.</p>

<p>In computer programs it&rsquo;s not even possible to obtain data which is truly random.
Usually it&rsquo;s just a <a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">function</a> which is hard to predict, and uses varying
inputs like time of data.
Better sources sample from physical motion, like air vibrations.
When random functions aren&rsquo;t random enough,
hackers can break <a href="https://en.wikipedia.org/wiki/Random_number_generator_attack">encryption</a>.</p>

<p>Just like the 2016 election, it doesn&rsquo;t take scrutiny to realize that most subjects of statistical study
are not actually driven by chance.</p>

<h3>When is Probability Useful?</h3>

<p>That is not to suggest that the entire field is misguided.
Probability itself is a sound mathematical model, central
to scientific inquiry.
But, if the required axiom of having a random variable
is virtually always false, then why should we ever hope to apply it
in the real world?</p>

<p>We have already talked about one use case.
Reasoning about a population from a sample.
Ignoring the missing principle of uniformity in nature, and whether
distributions are correct, this is still a helpful way to study
large groups.</p>

<p>Another use case is when there are just too many variables to understand.
When we have a cause and effect theory, but too many input variables,
it&rsquo;s overwhelming to use, so we settle for partial information.
Consider a dice roll.
Through classical mechanics we can understand gravity, the force of the throw, the friction in the air,
the impact on the table,
and with all this make an accurate description of the outcome.
But no human can capture all those parameters in a split second and apply them.
Without a controlled environment for study probability is a good model.</p>

<p>However, statistical physical models don&rsquo;t just making random assignments.
They tightly constrain the probability with deterministic mathematics,
such as using regular formulas for area and force. (3)
Furthermore, they never imply an underlying law of randomness.
Rather they are useful compromises with tolerable inaccuracies.</p>

<p>The most common use case for statistics,
and the one that needs to be scrutinized the most,
is when you don&rsquo;t really
understand cause and effect relationships.
Without a workable theory of nature, stats act as an adhoc placeholder.
Data gets mapped to distributions, and patterns get correlated,
without considering how the underlying objects works.</p>

<p>When this is your only option, it&rsquo;s probably better than nothing.
But, you have to be honest about the limitations.
You definitely shouldn&rsquo;t be surprised to find &ldquo;black swans&rdquo;
when all you did is plot a bell curve,
and hope everything else follows uniformly.</p>

<p>Consider two fields almost synonymous with stats; economics and psychology.
Does anyone have a clear picture of how markets function,
or the causes of inflation?
Can anyone give an account of why people think and believe what they do?
Of course not, and the popular theories change rapidly and dramatically.
There are are a lot of variables in these domains
too, so perhaps probability models are unavoidable.
But clearly they are lacking in explanatory theories.
Maybe a scientific theory isn&rsquo;t even possible.</p>

<h3>A Universal Way of Understanding</h3>

<p>In <a href="http://classics.mit.edu/Plato/republic.11.x.html">The Republic X</a>, Plato observes a related attitude in painting and poetry.
He describes their practice as giving an appearance of understanding through shallow, and quickly made presentations:</p>

<blockquote><p>The imitator, I said, is a long way off the truth, and can do all things because he lightly touches on a small part of them, and that part an image.
For example: A painter will paint a cobbler, carpenter, or any other artist, though he knows nothing of their arts; and, if he is a good artist, he may deceive children or simple persons, when he shows them his picture of a carpenter from a distance, and they will fancy that they are looking at a real carpenter.</p>

<p> And whenever any one informs us that he has found a man knows all the arts, and all things else that anybody knows [&hellip;] I think that we can only imagine to be a simple creature who is likely to have been deceived by some wizard or actor whom he met, and whom he thought all-knowing, because he himself was unable to analyze the nature of knowledge and ignorance and imitation.</p>

<p>And so, when we hear persons saying that the tragedians, and Homer, who is at their head, know all the arts and all things human, virtue as well as vice, and divine things too, for that the good poet cannot compose well unless he knows his subject, and that he who has not this knowledge can never be a poet, we ought to consider whether here also there may not be a similar illusion. Perhaps they may have come across imitators and been deceived by them</p></blockquote>

<p>Statistics students and professionals hold the same remarkable belief about their own field today.
They don&rsquo;t need to study any subject or &ldquo;applications&rdquo;, besides statistics itself.
Their statistical knowledge is immediately transferable to
understanding any problem thrown their way, whether in tech, business, politics, finance, or healthcare.
It&rsquo;s a universal framework for understanding.
This makes it the perfect career in the culture of randomness, and ironically
minimize the need to make predictions about the future.</p>

<p>But can you understand business without running one?
Can you understand voting patterns without understanding what
is happening in people&rsquo;s lives?
Can you understand psychology without thinking, observing, and talking to people?
Lack of basic knowledge and experience leave a lot to be desired.
In these areas, as in politics, we can conclude that probability and statistics are poor substitutes
for explanatory theory.</p>

<p><strong>Notes</strong></p>

<p>(1) Pachinko might be a good model to predict whether an INDIVIDUAL voter will make it to the voting
location, as intended. Each peg represents an event encouraging or discouraging them (medical emergencies, flat tires, relationship trouble).
But these don&rsquo;t happen very often and the vast majority of those determined follow through with their intentions.</p>

<p>(2) For an exploration of these ideas see Zero To One by Peter Thiel.</p>

<p>(3) For a specific examples see the derivations for the equations of <a href="https://en.wikipedia.org/wiki/Kinetic_theory_of_gases">motion of gasses</a>
or how probability is used to <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">estimating integrals</a>.</p>

 ]]></description>
</item>
<item>
<pubDate>Wed, 07 Oct 2020 00:00:00 </pubDate>
<title>Derivative Map Tool</title>
<guid isPermaLink="true">https://github.com/justinmeiners/derivative-map-tool</guid>
</item>
<item>
<pubDate>Sat, 26 Sep 2020 00:00:00 </pubDate>
<title>Understanding LINQ GroupBy</title>
<guid isPermaLink="true">https://justinmeiners.github.io/understanding-groupby/</guid>
<description><![CDATA[ 
<h1>Understanding LINQ GroupBy</h1>

<p><strong>09/26/20</strong></p>

<p>C# programmers are typically familiar with  <code>Select</code>, <code>Where</code>, and <code>Aggregate</code>, the LINQ equivalents of the core functional programming
operations <code>map</code>, <code>filter</code>, and <code>reduce</code>.
But the equally important <code>GroupBy</code> isn&rsquo;t as well understood or often used.
Although you may guess from the name, the problem <code>GroupBy</code> solves is the following:
Given a list of items, we want to arrange the items so that equal elements are together.</p>

<p>For example, given a list of <code>Student</code> objects,
we may want to group them by <code>student.teacher</code>, and then obtain a list of <code>student.id</code>
corresponding to each teacher.</p>

<p><code>GroupBy</code> is a beautiful piece functional programming which allows work to be specified in terms of &ldquo;what&rdquo;
should happen as opposed to &ldquo;how&rdquo; to do it.
It combines harmoniously with the other LINQ operations
leading to cleaner code that can be combined in modular ways.</p>

<h3>How to use GroupBy</h3>

<p>With the brief description above in mind,
let&rsquo;s examine the method <a href="https://docs.microsoft.com/en-us/dotnet/api/system.linq.enumerable.groupby?view=netcore-3.1#definition">signature</a>, which is a little hairy and intimidating:</p>

<pre><code>IEnumerable&lt;TResult&gt; GroupBy&lt;TSource, TKey, TElement, TResult&gt; (
    IEnumerable&lt;TSource&gt; source,
    Func&lt;TSource,TKey&gt; keySelector,
    Func&lt;TSource,TElement&gt; elementSelector,
    Func&lt;TKey,IEnumerable&lt;TElement&gt;,TResult&gt; resultSelector,
    IEqualityComparer&lt;TKey&gt; comparer
);
</code></pre>

<p>Ignore the generic types and focus on the 4 functions that must be provided:</p>

<ul>
<li><code>TKey keySelector(TSource item)</code> given an <code>item</code>, return a key which will be used to decide which group to place this <code>item</code>.
Usually this simply returns a field on <code>item</code>, but it can also generate a key, as we will see in a later example.</li>
<li><code>TElement elementSelector(TSource item)</code> given an <code>item</code>, return the value that will actually be stored in the group,
Often this returns the item or a field on the item.</li>
<li><code>TResult resultSelector(TKey key, IEnumerable&lt;TElement&gt; contents)</code> after elements are grouped together,
this function will be called for each group. The <code>key</code> is the identifier for this group and the <code>contents</code> is an enumerable
object containing the results. Most often you will just return <code>contents</code>, but you may want to store the result in a new class.</li>
<li><code>int comparer(..)</code> provide a function so that keys can be compared. This can be left off for key types like integer and string which are comparable.</li>
</ul>


<p>With an overview of the functions, the meaning of the 4 generic types is now clear:</p>

<ul>
<li><code>TSource</code> the items in the collection we will be grouping.</li>
<li><code>TKey</code> from each item a key will be extracted to identity which group it belongs to.</li>
<li><code>TElement</code> each item has the opportunity to be transformed before being put into a group.
 This is the type of that transformation.</li>
<li><code>TResult</code> The type representing each group. Typically a collection, but could be custom class.</li>
</ul>


<h3>Example 1: Each teachers' students.</h3>

<p>Once again, given a list of <code>Student</code> objects,
we may want to group them by <code>student.teacher</code>, and then obtain a list of <code>student.id</code>
corresponding to each teacher.</p>

<pre><code>List&lt;Student&gt; students = ...;

students.GroupBy(
   s =&gt; s.teacher,        // keySelector
   s =&gt; s.id,             // elementSelector
   (teacher, ids) =&gt; ids  // resultSelector
);

// Result: [ [id1, id2, ..], [id1, id2] ] 
</code></pre>

<h3>Example 2: A Histogram of Purchases</h3>

<p>Let&rsquo;s imagine we are building a personal finance application.
One report which might be useful is a histogram showing how many purchases
fall within each price range,
so $10.75 and $12.37 both fall within then $10-20 range.
We want to find out the number of purchases, and the total value of these purchases.
We will use a tuple to store these calculations for each resulting bucket.</p>

<pre><code>struct Purchase {
   double amount;
   ...
}

List&lt;Purchase&gt; purchases = ...;

purchases.GroupBy(
   p =&gt; (int)Math.Floor(p.amount / 10.0),    // keySelector
   p =&gt; p.amount,                            // elementSelector
   (bucket, amounts) =&gt; {                    // resultSelector
      return Tuple.Create(
         bucket,
         amounts.Count(),
         amounts.Sum()
      );
   }
)

// Result:
// [ (bucket, number of purchases, total purchase amount), ... ]
</code></pre>

<p>Assuming the range of buckets was known up front, one could simply allocate an array with the proper
number of buckets and place each item directly.
But, this implementation is very robust, especially for sparse data sets, and
forgoes the need to translate our bucket values to indices.</p>

<h3>Deep Dive: Implementing GroupBy with Sort and Merge</h3>

<p>That may be all you want about <code>GroupBy</code> now.
Keep reading if you are curious about how it actually works or wonder
about it&rsquo;s performance.
Grouping elements appears to be an inherently complex task, at first.
An initial idea might be to loop through each item, and then loop through all the other items to find matches.
This works, but is an <code>O(n^2)</code> algorithm so we want to try a little harder.</p>

<p>The next natural implementation is to create
a dictionary in which each key contains the results of a group. Iterate over each item in the list once,
and put it in it&rsquo;s proper group.
This works just fine, but it seems a little kludgy.
Dictionaries are rarely uniform.
For each key you must check if a group is there and if not, create it,
then at the end you have to iterate over a dictionary to put the final groups
into an appropriate structure.</p>

<p>We can actually do bit better in elegance and often performance (not time complexity) as well.
The idea is to <code>sort</code> elements by their key (<code>O(n log n)</code>).
After sorting, all the equal elements that should be grouped together will be next to each other.
We can then apply a reduce-like operation to merge adjacent neighbors.</p>

<pre><code>public static IEnumerable&lt;TResult&gt; GroupBy&lt;TSource, TKey, TElement, TResult&gt; (
     IEnumerable&lt;TSource&gt; source,
     Func&lt;TSource,TKey&gt; keySelector,
     Func&lt;TSource,TElement&gt; elementSelector,
     Func&lt;TKey,IEnumerable&lt;TElement&gt;,TResult&gt; resultSelector
 ) {
     var sorted = source.Select(item =&gt; ValueTuple.Create(keySelector(item), elementSelector(item)))
             .OrderBy(tup =&gt; tup.Item1);

     var start = sorted.First();
     var currentGroup = new List&lt;TElement&gt;()
     {
         start.Item2,
     };

     var groupKey = start.Item1;

     foreach (var x in sorted.Skip(1)) {
         if (x.Item1.Equals(groupKey)) {
           currentGroup.Add(x.Item2);
         } else {
             yield return resultSelector(groupKey, currentGroup);
             currentGroup = new List&lt;TElement&gt;()
             {
                 x.Item2
             };
             groupKey = x.Item1;
         }
     };

     yield return resultSelector(groupKey, currentGroup);
 }
</code></pre>

<p>This pattern of &ldquo;sorting and merging&rdquo; is a powerful one.
See <a href="https://github.com/psoberoi/stepanov-conversations-course/blob/master/styles/fortran4.cpp">Stepanov&rsquo;s bigrams</a> for an additional application.
Note that this is almost certainly not how LINQ is implemented (we don&rsquo;t give it a key comparer, just equality test),
but is instructive.</p>

<h2>More About LINQ</h2>

<p>LINQ is a fantastic example of the usefulness of domain specific languages (DSL).
It&rsquo;s a mini language designed for manipulating and transforming data in collections,
taking advantage of the strong theoretical foundations of functional programming.
But the benefits of pure functions come with serious constraints.
With LINQ you can take advantage of them when they make sense and avoid them when they don&rsquo;t. (Common Lisp actually does the reverse,
the default style is somewhat functional and a &ldquo;<a href="http://clhs.lisp.se/Body/m_prog_.htm#prog">goto style</a>&rdquo; DSL is available for writing
performance code.)</p>

<p>Although languages like Lisp are built around the idea of <a href="http://www.paulgraham.com/onlisp.html">constructing DSLs</a>,
it&rsquo;s difficult to see how a language with roots in C could be made similarly malleable.
In fact, a great deal of runtime and compiler infrastructure
needed to be added to C# in order to support LINQ, including <code>lambdas</code>
anonymous objects, etc. Jon Skeet suspects the C# team had an idea to completely
generalize data access,
and then built up a chain of dependent features to reach this goal.
To learn more about this process and the ideas behind LINQ
I recommend Jon Skeet&rsquo;s book <a href="https://www.manning.com/books/c-sharp-in-depth-fourth-edition?utm_source=affiliate&amp;utm_medium=affiliate&amp;a_aid=11033&amp;a_bid=569211b4">C# in Depth</a>.
I think you will enjoy it even if you are not interested in C# itself, as it is a joyful deep dive into practical language design and theory.</p>

 ]]></description>
</item>
<item>
<pubDate>Sat, 12 Sep 2020 00:00:00 </pubDate>
<title>Gesture Recognition with Line Integrals</title>
<guid isPermaLink="true">https://justinmeiners.github.io/gesture-recognition/</guid>
</item>
<item>
<pubDate>Sun, 24 May 2020 00:00:00 </pubDate>
<title>Boring Benefits of Lisp</title>
<guid isPermaLink="true">https://justinmeiners.github.io/boring-benefits-of-lisp/</guid>
<description><![CDATA[ 
<h1>Boring Benefits of Lisp</h1>

<p><strong>10/07/20</strong></p>

<p>Lisp (both Common Lisp and Scheme) advocates are famous for having extravagant reasons for why Lisp is their favorite language.
You might have heard claims that it&rsquo;s the <a href="http://www.paulgraham.com/avg.html">most powerful language</a>,
due to feature like <a href="http://gigamonkeys.com/book/macros-defining-your-own.html">macros</a> or <a href="https://en.wikipedia.org/wiki/Homoiconicity">homioconicty</a>.
Certainly, Lisp has no shortage of beautiful and thought provoking ideas,
but due to it&rsquo;s influence, most of its benefits have now been included
in modern languages.
But, fancy language abstractions <a href="https://justinmeiners.github.io/think-in-math/">don&rsquo;t appeal to me</a>,
so the whole meta-programming thing isn&rsquo;t compelling.
I am now interested in it for very simple and practical reasons;</p>

<ol>
<li>Common Lisp and Scheme are both fully standardized language with specifications.
Consequently, they are well understood, cross-platform, and has multiple implementations, including several with free licenses.</li>
<li>Lisp has great documentation, books, and learning resources. SICP is &ldquo;the book&rdquo; for Scheme.
Common Lisp has several good ones.</li>
<li>Lisp is mature and extremely stable. Code be written once, and run again years later, without modification.</li>
<li>Lisp implementations are reasonably fast. The true believers claim Common Lisp is as fast as C. In general it&rsquo;s not even close, but for a high-level, dynamic, language, <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/lisp.html">it&rsquo;s pretty fast</a>.</li>
<li>Lisp is well designed and follows solid computer science principles.
It has a focused selection of features and an elegant evaluation model which make it easy to
write and compose functionality.</li>
</ol>


<p>You might think, this list of features isn&rsquo;t remarkable or unique to Lisp,
and you would be correct.
In fact, this pattern was actually established by ANSI C first, and later adopted by Lisp.
But, the surprising thing is how few languages since have followed it&rsquo;s lead.
Of course each item in this list is somewhat of a spectrum; some languages do more than others,
but very few follow it to a degree that can be asserted with confidence.
Usually, you can say a language somewhat satisfies it, followed by a list of ugly qualifications.</p>

<p>Take Python for example. It&rsquo;s well designed, has great resources, is fairly stable.
It has an <a href="https://norvig.com/python-lisp.html">elegant design</a> inspired by Lisp.
But, is it standardized? Kind of, they have a spec, but it has one defacto implementation.
The CPython project does whatever they want, and this defines what the language is, forcing others to follow along.
Alternative implementations exist, but they all have compatibility compromises.
Python is also not very fast. You can use a <a href="https://www.pypy.org">JIT</a> implementation which makes it tolerable,
but that has its own quirks, and you can&rsquo;t use many libraries with it.</p>

<p>So are Lisp and C the only languages that are based on performance, robust standards, and free software?
No, but there aren&rsquo;t as many as you think.
Lisp just happens to be one of them, and it&rsquo;s a design that I enjoy using and learning about.</p>

<p><strong>Update: 4/30/21</strong>: I recommend the article <a href="https://begriffs.com/posts/2020-08-31-portable-stable-software.html">&ldquo;Tips for stable and portable software&rdquo;</a> which  discusses principles of stability, not only at the language level, but in the whole stack. It shows
practical examples of things to look for in programming languages and Unix environments.</p>

 ]]></description>
</item>
<item>
<pubDate>Sun, 12 Apr 2020 00:00:00 </pubDate>
<title>10 Hard Decisions: A Model for Programmer Productivity</title>
<guid isPermaLink="true">https://justinmeiners.github.io/10-hard-decisions/</guid>
<description><![CDATA[ 
<h1>10 Hard Decisions: A Model for Programmer Productivity</h1>

<p><strong>09/26/20</strong></p>

<p>How do you ensure that programmers are using time effectively?
What can you do to help them be productive?
Managers think a lot about these questions.
But too often they get the answers wrong despite being very earnest and smart.
This is because they have an unexamined mental model of how programming work is actually done.
With an inaccurate model, managers optimize the wrong things and make bad decisions
which would otherwise be good decisions in other work environments.</p>

<p>To understand the idea, let&rsquo;s look at a few examples of how productivity varies between jobs.
Simple manual labor, like digging holes, is easy to understand.
Productivity comes down to moving as much dirt per minutes as possible.
Progress is very measurable, and it easy to think about how to improve it.
As a manager, you might ensure there is a rotation of workers digging at all time,
and that everyone has the best tools possible.
However that looks very different than the working model for a professional baseball scout.
A scout can take a few years of evaluating players just to find one great pro.
There aren&rsquo;t simple metrics you can use to measure their progress.
It&rsquo;s hard to tell exactly what productive work should look like.
Exposure to a lot of players can help, but ultimately it comes down to a few big choices.</p>

<p>What kind of work is programming most like?
In my experience (especially in agencies), the typical
model of a programmer is an expensive factory machine.
This is the kind of machine that occupies a lot of space and attention in the factory.
It&rsquo;s operation is fragile, so it needs to be constantly checked on and maintained.
But most importantly, every minute that it is not working on jobs is expensive money wasted.
The primary metric for ensuring it stays productive is utilization time, even if that means spending time
on throw away jobs.</p>

<p>Managers understand this work model very well, perhaps they read &ldquo;The Goal&rdquo; in school.
They look for metrics to identify throughput and bottlenecks and do lots of shuffling of programmers
around to ensure maximum utilization.
In it&rsquo;s most exaggerated form this led managers to obsess over metrics &ldquo;lines of code per week&rdquo;.
Although we have moved beyond that.
The underlying thinking about what&rsquo;s going on has changed much.
We just think about programmers as feature producers.
The task the programmer needs to finish programmer needs to sit at the computer and type in the project.</p>

<p>Here is a common example of this thinking: Bob is busy finishing task 1 and then will start project 2.
Alice has a bit of downtime before she starts project 3.
Let&rsquo;s have Alice get started on project 2, then we can
have her hand off the incomplete work to Bob.
In a few weeks when he is ready, Alice can move on to project 3 and leave Bob with a head start.</p>

<p><img src="timeline.png" alt="sample timeline" /></p>

<p>This seems like smart move.
We made sure that Alice and Bob&rsquo;s valuable time was well used, right?</p>

<h2>A better Model</h2>

<p>To see why this decision may actually waste time, we need
a better understand what programming is like.
The model I will describe is not perfect, it&rsquo;s a great simplification, but
its a rough estimation that&rsquo;s far more accurate than the factory machine one.
That better model is the following:
<strong>each project consists of making 5-15 difficult software decisions</strong>.
All of these decisions are of course somewhat dependent on one another.
Each of those decisions that is made well, will lead to shipping the project,
Each decision made wrong, will cause delays and waste time, now or down the road.
That&rsquo;s all. That&rsquo;s the description of the job programmers need to do,
and how they occupy their time.
If programmers knew what to write beforehand, they could type it in no time!
The time they spend fiddling at the keyboard is actually gather information
through experiment and observation, in order to make decisions.</p>

<p>As a manager, the way to improve productivity
is to simply think about what would help you make hard decisions.
Here are some simple ideas that become immediately obvious, given this clearer model:</p>

<ul>
<li><p>Give programmers as much possible information about the project as early as possible.
It&rsquo;s difficult to make decisions with incomplete information.
When you get ideas early they can sit in the back of your mind and get some sleep
time.  Organize information so its easy to process.</p></li>
<li><p>Protect programmer time from interruptions.
Design large blocks of uninterrupted time.
Have quiet working environments.
It&rsquo;s hard to make good decisions without focus.
Other writers have written extensively about the cost of &ldquo;<a href="http://www.paulgraham.com/makersschedule.html">context switching</a>&rdquo;
when you have to forget about what you were thinking and start thinking about something new.</p></li>
<li><p>Ensure programmers have a clear understanding of project priorities and goals.
Clearly communicate expectations and requirements.
Making good decisions requires making trade-offs.
Unclear priorities will lead to programmers making bad decisions.
Changing priorities will invalidate previously good decisions.</p></li>
<li><p>It&rsquo;s OK if a programmer is not at their keyboard writing code.
Taking a walk or a rest is often productive time.
Distracting activities like watching a lot YouTube are not.</p></li>
<li><p>Don&rsquo;t treat programmers interchangeably.
Give them ownership of specific project pieces so they can make long term decisions and build on previous ones.
Would you hire a separate architect to design each room of a house?</p></li>
<li><p>Ensure your incentive structure rewards good decision making.
Do you reward band-aid approaches? Are programmers just trying to get it &ldquo;over the fence&rdquo;
so someone else has to deal with it? Are programmers are the receiving side of the fence?</p></li>
<li><p>Going through a crunch? <a href="https://en.wikipedia.org/wiki/Brooks%27s_law">Don&rsquo;t add people</a> to a late project. Throw more money and effort at building an focused environment to making decisions. Can you pay someone to errands at home so the programmer doesn&rsquo;t have to think about them? Can you bring their family to the office for a lunch? (1)</p></li>
</ul>


<p>This is just a short list of suggestions and you can probably can come up with even better
ideas.
What&rsquo;s important is that as we think deeper about this model common productivity mistakes become apparent. (2)</p>

<p>Let&rsquo;s analyze the Bob and Alice example again using this model.
Did we help Alice and Bob make decisions or hurt them?
Recall, Alice starts working on task 2 for a few weeks, to get it started for Bob.
Then Bob picks it up, and Alice moves elsewhere.
In our rough model we can assume that when Alice finishes, she has made 2 of the 15 decisions, and has some vague ideas about 5 others.
The manager arranges a meeting for Alice to inform Bob of her work and she is swept off to task 3.
Now Bob has to start thinking about task 2 from scratch, but not only that,
he has to figure out what Alice was up to, whether the decisions she made were right,
and what direction she was heading to address the other 13 decisions.
This added information causes Bob to take more time than if he had started on his own.
In practice, its even worse because Alice and Bob will need to continue to take time from each other to coordinate about what Alice had in mind. (3)</p>

<h2>Smarter Alternatives</h2>

<p>You might say, &ldquo;that makes sense, but what am I supposed to do? Let Alice sit around for a week, wasting that time with an urgent project?&rdquo;. Let&rsquo;s use the model to work through better options.
In this scenario Alice is scheduled to eventually own project 3.
A better option would be to just let task 2 sit until Bob is ready, and get her stated on her project, even though it may be less urgent.
Alice will be able to spend more time thinking about her project, and leave Bob with a fresh slate.</p>

<p>&ldquo;But this is really urgent, I don&rsquo;t even want to think about project 3 yet&rdquo;.
The next alternative would be to identify decisions which Alice can make which have the least impact on the other work Bob needs to do.
One would be having Alice better define the requirements and organize resources for this project.
This will only help Bob make his decisions.
Another idea is to have Alice work on &ldquo;black boxes&rdquo; that Bob doesn&rsquo;t need to know how they work.
Maybe Alice has some special knowledge and can write a hard function he expects to use.
What you absolutely don&rsquo;t want to do is interrupt Bob and have him start thinking about what decisions Alice could be making, while he is making his decisions.
That doesn&rsquo;t make any sense!</p>

<h2>Notes</h2>

<ol>
<li><p>I don&rsquo;t think programmers <em>deserve</em> any of these special treatments over other jobs.
I am not suggesting that programmers should have special privileges or are somehow more important than other kinds of work.
It&rsquo;s all about the productivity model for the kind of work they do.</p></li>
<li><p>In video game production, art tends to be easy to outsource while programming is notoriously difficult and often unsuccessful. Perhaps this is because 3D art production can be better fit into a factory production productivity model.</p></li>
<li><p>One reason &ldquo;just write it from scratch&rdquo; is so appealing for programmers is that
they get to make their own decisions, instead of figuring out why others made (often bad) decisions.
Who wants to live with someone elses choices?
For large existing codebases this represents a lot of invested knowledge, so its usually bad,
but for a new task, Why would you rob a programmer of this opportunity by playing
musical chairs?</p></li>
</ol>


 ]]></description>
</item>
<item>
<pubDate>Thu, 23 Jan 2020 00:00:00 </pubDate>
<title>Keeping Up-to-Date</title>
<guid isPermaLink="true">https://justinmeiners.github.io/keeping-up-to-date/</guid>
<description><![CDATA[ 
<h2>Keeping Up-to-date</h2>

<p><strong>1/23/2020</strong></p>

<p>The software industry is synonmous with rapid innovation.
New discoveries require changing to better ways of doing things.
For programmers to stay relevant they need to constantly keep up on the latest technologies,
otherwise their skills will go out of date.
At least, that&rsquo;s what I&rsquo;ve been told,
but the more I study computer science from 50 years ago,
the more I find &ldquo;new&rdquo; ideas are actually old.
Perhaps the changes we see are only permutations
of underlying ideas.</p>

<p>In many obvious ways software does change.
Languages, libraries, and tools get replaced overtime.
Nobody is writing desktop applications in assembly anymore.
Many developers who wrote Windows applications moved on to mobile or web.
But rarely are these changes due to a technological advancement
or dramatic in change in how things are done, just new forms
of popular products, or improved hardware that gives us some wiggle room.</p>

<p>A good engineer can adapt to these changes, in the same way they can switch
to a company which uses different tools and processes.
Learning Python after C# should be easy, because you understand programs, not the syntax.
Moving to a language with significant design differences like Haskell requires a broader
understanding of functions and computation.
Now imagine there was similar knowledge that helped you approach every computer problem and perhaps
all of nature.
The knowledge I am describing is math and science!
For software it consists of <a href="https://teachyourselfcs.com/">computer science subjects</a> like OS theory, algorithms, software design, logic,
and calculus.
Because many programmers lack this, they struggle with change, and feel pressure
to keep up with trends. (1)</p>

<p>Don&rsquo;t believe me?
Take a look at the hottest &ldquo;new&rdquo; areas of tech
AI/machine learning, blockchain, and big data/data science.
What barriers make it difficult for programmers to get into them and be successful?
For machine learning the answer is a bit of linear algebra and multivariable calculus,
evidenced by the many blog post promising to get readers up to speed.
This math is (should be) covered by every computer science degree
and has remained the same for at least 50 years,
down to the presentations and illustrations used to <a href="https://www.youtube.com/watch?v=wsOoClvZmic">teach it</a>.
<a href="https://justinmeiners.github.io/neural-nets-sim/">Neural networks</a> themselves have been around for a <a href="https://en.wikipedia.org/wiki/Perceptrons_(book)">long time</a>,
although less mainstream.</p>

<p>For <a href="https://justinmeiners.github.io/tiny-blockchain/">blockchain</a> the essentials are an understanding of cryptography,
and peer-to-peer networking. That doesn&rsquo;t even mean fancy understandings
of elliptic curves or number theory, just a solid grasp of hashes,
signatures, and asymetric key encrpytion.
Distributed systems is also a mature field of computer science.</p>

<p>Data science might be the most accessible.
A solid understanding of introductory probablity and statistics and databases
combined with a few tools such as linear regression, and polynomial interpolation
might be enough for 90% of applications.
But it&rsquo;s going to be really hard, if you can&rsquo;t understand a wikipedia page about <a href="https://en.wikipedia.org/wiki/Horner%27s_method">polynomials</a>.</p>

<p>I don&rsquo;t mean to suggest that these skills make an expert.
General theory is a longshot from research or novel contributions to the field.
Nor is it sufficient to be a good programmer; learning linear algebra does not immediately
make you good at writing machine learning programs (and scientists write some terrible code!).
Rather this is the &ldquo;hard stuff&rdquo; that prevents programmers from getting into these fields.
Once you know it, the other details are approachable. (2)</p>

<p>If you analyze other software advances from the past, from database theory
to graphics, you will find similar applications
of rather unextraordinary math and science.
Many programmers ask, how I can I predict what skills will be important in the future?
What do I need to learn to have a successful career?
Few can predict what specific trends will take off, but I
bet whatever is important in the future is going to require understanding
those broad areas of computer science.
Keep practicing and specializing in the area you work in, but if you regularly refresh and
broaden your base of fundamentals
you will be prepared to learn anything new that starts to look interesting. (3)</p>

<p>Understanding fundamentals gives programmers another significant advantage.
They know what problems have been solved before.
It&rsquo;s unlikely you will remember every detail, but can say
&ldquo;I&rsquo;ve heard of this before&rdquo; and know where to learn more.
Consider how many new tools are bad solutions to problems solved by simple bash scripts,
of which the author was ignorant of.
This is just a tiny represenative of how much duplication and complexity
programmers are adding because they don&rsquo;t know whats already there.</p>

<p>It may sound as though I am advocating a kind of tech hipsterism;
everything interesting has already been done so we might
as well stop looking for new things. Rather I am arguing
that we will be able make more advancements, if we better
understand the big ideas behind what has come before.</p>

<p>Properly understood this flips the progress narrative of technology on it&rsquo;s head.
We don&rsquo;t have to chase headlines and blog posts about the latest frameworks
and build tools.
Nor do we have to guess which technologies might suddenly become useful, like day trading stocks.
That&rsquo;s for evangelists and IT consultants.
The foundations for our next ideas has have already been built by generations of smart people.
It&rsquo;s all written down waiting to be dusted off and rediscovered.
Rather than building and encyclopediac knowledge of novelties
 and press releases,
we continuosuly revisit the same core subjects, over and over, in pursuit of mastery.</p>

<ol>
<li><p>It doesn&rsquo;t have to come from a university, and a majority
of those who study STEM seem to lose it after going through the motions.</p></li>
<li><p>A great example of this is the
popular book <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html">SICP</a>. In just a few hundred
pages it covers several major areas of CS and more
than most programmers understand in their whole career.
It&rsquo;s able to do that because it&rsquo;s not written
for a general audience.
It relies on the reader having a solid undergraduate understanding of another
area of math or science as an MIT student would have.</p></li>
<li><p>Experts in a very particular system
(for example OpenCL or <a href="https://v8.dev/blog">V8 internals</a>) can be extremely valuable.
But, most of them only get that level of depth with a solid
understanding of fundamental computer science.
Expertise is also a subject for another day.</p></li>
</ol>


 ]]></description>
</item>
<item>
<pubDate>Sun, 28 Jul 2019 00:00:00 </pubDate>
<title>Write Your Own Proof-of-Work Blockchain</title>
<guid isPermaLink="true">https://justinmeiners.github.io/tiny-blockchain/</guid>
</item>
<item>
<pubDate>Sat, 29 Jun 2019 00:00:00 </pubDate>
<title>text2image</title>
<guid isPermaLink="true">https://github.com/justinmeiners/text2image</guid>
</item>
<item>
<pubDate>Sat, 08 Jun 2019 00:00:00 </pubDate>
<title>Think in Math. Write in Code.</title>
<guid isPermaLink="true">https://justinmeiners.github.io/think-in-math/</guid>
<description><![CDATA[ 
<h2>Think in Math. Write in Code.</h2>

<p><strong>6/8/19</strong></p>

<p>Programmers love to discuss programming languages.
We not only debate their technical merits and aesthetic qualities,
but they become integrated into our personal identities,
along with the values and traits that we associate with them.
Some even defend a form of <a href="https://en.wikipedia.org/wiki/Linguistic_determinism">Linguistic Determinism</a> that thinking is confined to what the language
makes typable.</p>

<p>Since we spend so much time writing code, a keen interest in language design is justified.
However, the character of these discussions suggests that we think of them as much more,
and have perhaps forgotten their primary role.
Programming languages are <em>implementation tools</em> for instructing machines, not <em>thinking tools</em>
for expressing ideas.
They are strict formal systems riddled with design compromises and practical limitations.
At the end of the day, we hope they make controlling computers bearable for humans.
In contrast, thoughts are best expressed through a medium which is free and flexible.</p>

<h2>Thinking in Math</h2>

<p>The natural language which has been effectively used for thinking about computation, for thousands of years, is mathematics.
Most people don&rsquo;t think of math as free or flexible.
They think of scary symbols and memorizing steps to regurgitate on tests.
Others hear math and think category theory, lambda calculus, or other
methods of formalizing computation itself, but these are hardly
necessary for programming itself.</p>

<p>I hope readers of this article have had a better experience regarding what math is about, such as a graph theory, algorithms, or linear algebra course;
the kind that involves logic and theorems, and is written in prose with a mix of symbols (most symbols weren&rsquo;t even invented until the <a href="https://en.wikipedia.org/wiki/History_of_mathematical_notation#Symbolic_stage">16th century</a>).
This kind of math is about creating logical models to understand real world problems, through careful
definitions and deductions.
If you don&rsquo;t have a clear idea of what this looks like I recommend <a href="https://www.amazon.com/Introduction-Graph-Theory-Dover-Mathematics/dp/0486678709">Trudeau</a>,
<a href="https://www.fm2gp.com">Stepanov</a>, or <a href="https://www.amazon.com/Introduction-Algorithms-Creative-Udi-Manber/dp/0201120372">Manber</a>.</p>

<p>Math allows you to reason about logical structures, free from other constraints.
This is also what programming requires: creating logical systems to solve problems.
Take a look at the basic pattern for programming:</p>

<ol>
<li>Identify a problem</li>
<li>Design algorithms and data structures to solve it</li>
<li>Implement and test them</li>
</ol>


<p>In practice, work is not so well organized as there is interplay between steps.
You may write code to inform the design.
Even so, the basic pattern is followed over and over.</p>

<p>Notice that steps 1 and 2 are the ones that take most of our time, ability, and effort.
At the same time, these steps don&rsquo;t lend themselves to programming languages.
That doesn&rsquo;t stop programmers from attempting to solve them in their editor, but they end up with code that is muddled, slow, or that solves the wrong problem.
It&rsquo;s not that programming languages aren&rsquo;t good enough yet.
It&rsquo;s that <em>no formal language</em> could be good at it.
Our brains just don&rsquo;t think that way.
When problems get hard, we draw diagrams and discuss them with collaborators.</p>

<p>Ideally, steps 1 and 2 are solved first, and only then will a programming language be used to solve step 3.
This has an added benefit of transforming the implementation process.
With a mathematical solution in hand, you can then focus on choosing the best representation and implementation, and writing better code, knowing what the end goal will be.</p>

<h2>Implementation Concerns</h2>

<p>Why are programming languages burdensome thinking tools?
One reason is that writing code is inseparably connected with implementation concerns.
A computer is a device that must manage all kinds of tasks and while being bound by physical
and economic constraints.
Think about all the considerations for writing a simple function:</p>

<ul>
<li>What inputs should I provide?</li>
<li>What should they be named?</li>
<li>What types should they be? (Even dynamically typed languages must consider types, it&rsquo;s just implicit.)</li>
<li>Should I pass them by value or by reference?</li>
<li>What file should I put the function in?</li>
<li>Should the result be reused, or is it fast enough to recalculate it every time?</li>
</ul>


<p>The list can go on. The point is that these considerations have nothing to do with what the function does.
They distract from the problem the function is trying to solve.</p>

<p>Many languages aim to hide details such as these, which is helpful, especially for mundane tasks.
However, they cannot transcend their role as an implementation tool.
SQL is one of the most successful examples of this, but it is ultimately concerned with implementation concerns such as tables, rows, indices, and types.
Because of this, programmers still design complicated queries in informal terms, like what they want to &ldquo;get,&rdquo; before writing a bunch of <code>JOIN</code>s.</p>

<h2>Inflexible Abstractions</h2>

<p>Another limitation of programming languages is that they are poor abstraction tools.
Typically, when we discuss abstraction in engineering, we mean hiding implementation details.
A complex operation or process is packaged into a &ldquo;black box&rdquo; with its contents hidden and well-defined inputs and outputs exposed.
Accompanying the box is fictional story that explains what it does, in a greatly simplified way.</p>

<p><img src="black-box.gif" alt="black box picture" /></p>

<p>Black boxes are essential for engineering large systems since the details are too overwhelming to hold in your head.
They also have many well-known limitations.
A black box <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">leaks</a> because its brief description cannot completely determine its behavior.
The opaque interfaces introduce <a href="https://www.youtube.com/watch?v=lHLpKzUxjGk">inefficiencies</a>, like duplication and fragmented design.</p>

<p>Most importantly for problem-solving, black boxes are rigid.
They must explicitly reveal some dials and knobs, and hide others,
committing to a particular view about what it is essential to expose to the user,
and what is noise.
In doing so, they present a fixed level of abstraction which may be too high-level or too low-level for the problem,
As an example, a high-level web server may provide a terrific interface for serving JSON, but be useless if one wants an interface for serving incomplete data streams, such as output from a program.
In theory, you can always look inside the box, but in code, the abstraction level at any one time is fixed.</p>

<p>In contrast, the word abstraction in math is nothing like hiding information.
Here, abstraction means extracting the essential features or characteristics of something, in relation to a particular context.
Unlike black boxes, no information is hidden.
They don&rsquo;t leak in the same way.
You are encouraged to adjust to the right level of abstraction and quickly jump between perspectives.
You might ask:</p>

<ul>
<li>Is this problem best represented as a table? Or, a function?</li>
<li>Can I look at the whole system as a function?</li>
<li>Can I treat this collection of things as a single unit?</li>
<li>Should I look at the whole system or a single part?</li>
<li>What assumptions should I make? Should I make them stronger or weaker?</li>
</ul>


<p>Just look at the many ways of looking at a function:</p>

<p><a href="https://www.youtube.com/watch?v=ACZDnF8-9Ks"><img src="functions.gif" alt="function representations" /></a></p>

<p>Thinking in math allows one to use whichever brings the most clarity at any moment.</p>

<p>It turns out most abstract concepts can be understood from many perspectives, just like functions.
Studying math provides one with a versatile toolbox of perspectives for studying all kinds of problems.
You might first describe a problem with a formula, and then switch to understanding it <a href="https://en.wikipedia.org/wiki/Felix_Klein#Erlangen_program">geometrically</a>,
then recognize some group theory (abstract algebra) is at play, and
all of this combines to give insight and understanding.</p>

<p>To summarize, programming languages are great engineering tools for assembling black boxes;
they provide functions, classes, and modules, all of which help wrap up code into nice interfaces.
However, when trying to solve problems and design solutions, what you actually want is the math kind of abstraction.
If you try to think at the keyboard, the black boxes available to you will warp your view.</p>

<h2>Problem Representation</h2>

<p>Just as programming languages are rigid in their ability to abstract, they also are rigid in how they represent data.
The very act of implementing an algorithm or data structure is picking <em>just one</em> of the many possible ways to represent
something; along with all the trade-offs that come with it.
It is always easier to make trade-offs when one has use cases in mind and understand the problem well.</p>

<p>For example, graphs (sets of vertices and edges) appear in many programming problems such as internet networks, pathfinding, and social networks.
Despite their simple definition, choosing how to represent them is hard and varies greatly depending on use case:</p>

<p><img src="graph.gif" alt="math graph" /></p>

<ul>
<li><p>The one which most closely matches the definition:<br/>
<code>vertices: vector&lt;NodeData&gt; edges: vector&lt;pair&lt;Int, Int&gt;&gt;</code>
(The vertices can be removed if you only care about connectivity.)</p></li>
<li><p>If you want to traverse a node&rsquo;s neighbors quickly, then you probably want a node structure:<br/>
<code>Node { id: Int, neighbors: vector&lt;Node*&gt; }</code></p></li>
<li><p>You could use an <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>. Where each row stores the neighbors of a particular node:
<code>connectivity: vector&lt;vector&lt;int&gt;&gt;</code> and the nodes themselves are implicit.</p></li>
<li><p>Pathfinding algorithms often work on graphs implicitly from a board of cells:<br/>
<code>walls: vector&lt;vector&lt;bool&gt;&gt;</code>.</p></li>
<li><p>In a peer-to-peer network, each computer is a vertex and each socket is an edge.
The entire graph isn&rsquo;t even accessible from one machine!</p></li>
</ul>


<p>Math allows you to reason about the graph itself, solve the problem, and then choose an appropriate representation.
If you think in a programming language, you cannot delay this decision as your first line of code commits to a particular representation.</p>

<p>Note that the graph representations are too diverse to wrapped up in a polymorphic interface.
(Consider again the graph representing a computer network, like the entire internet.)
So creating a completely reusable library is impractical.
It can only work on a few types, or force all graphs into an inappropriate representation.
That doesn&rsquo;t mean libraries or interfaces aren&rsquo;t useful.
Similar representations are needed again and again (like <code>std::vector</code>),
but you cannot write a library which encapsulates the concept of &ldquo;graph&rdquo; once and for all.
A simple generic or interface with a few types in mind is appropriate.</p>

<p>As a corollary, programming languages should focus primarily on being useful implementation tools,
rather than theoretical tools.
A good example of modern language feature which does this is async/await.
It&rsquo;s not hiding away complex details or introducing new conceptual theory.
It takes a common practical problem and makes it easier to write.</p>

<p>Thinking in math also makes the &ldquo;C style&rdquo; of programming more appealing.
When you understand a problem well, you don&rsquo;t have to build up layers of framework and abstraction
in anticipation of &ldquo;what if&rdquo;.
You can write a program tailor made to the problem, with carefully chosen trade-offs.</p>

<h2>Example Project</h2>

<p>So what does thinking in math look like?
For this section you may have to read a bit more slowly and carefully.
Recently, I worked on an API at work for pricing cryptocurrency for merchants.
It takes into account recent price changes and recommends that merchants charge a higher price during volatile times.</p>

<p>Although we did some homework on the theory, we wanted to empirically test it to see how it performed during various market conditions.
To do so, I designed a bot to simulate a merchant doing business with our API, to see how it performs.</p>

<p><strong>BTC/USD (1 day)</strong></p>

<p><img src="btc-usd.gif" alt="btc usd" /></p>

<h3>Preliminaries</h3>

<p><strong>Definition:</strong> The <strong>exchange rate</strong> <code>r(t)</code> is the market rate of <code>fiat/crypto</code>.</p>

<p><strong>Definition:</strong> The <strong>merchant rate</strong> <code>r'(t)</code> is the modified exchange rate which the merchant is advised to charge customers.</p>

<p><strong>Definition:</strong> When a customer buys an item, we call that event a <strong>purchase</strong>.
A purchase consists of the price in fiat and a time. <code>p = (f, t)</code>.</p>

<p><strong>Theorem:</strong> The amount of crypto for a purchase is found by applying the modified exchange rate
<code>t(p) = p(1) / r'(p(2))</code>.</p>

<p><strong>Proof:</strong> <code>p(1) / r'(p(2)) = fiat / (fiat/crypto) = fiat * crypto/fiat = crypto</code></p>

<p><strong>Definition:</strong> When the merchant sells their crypto holdings, we call that event a <strong>sale</strong>.
A sale consists of an amount in crypto and a timestamp. <code>s = (c, t)</code>.</p>

<p><strong>Theorem:</strong> The amount of fiat the merchant obtained from a sale is found by applying the exchange rate to the sale <code>g(s) = s(1) * r(s(2))</code>.</p>

<p><strong>Proof:</strong> <code>s(1) * r(s(2)) = crypto * (fiat/crypto) = fiat</code></p>

<p><strong>Definition:</strong> The <strong>balance</strong> of a set of purchases and sales is the difference between all purchase crypto amounts and all sale crypto amounts.
 <code>b(P, S) = sum from i to N of t(p_i) - sum from j to M of s_j(1)</code></p>

<p>Note that <code>b(P, S) &gt;= 0</code> must always hold.</p>

<p><strong>Definition:</strong> The <strong>earnings</strong> of a set of purchases and sales is the difference between sale fiat amounts and purchase fiat amounts.
<code>e(P, S) = sum from j to M of g(s_j(1)) - sum from i to N of p_i(1) &gt;= 0</code>.</p>

<h3>Objective</h3>

<p><strong>Definition:</strong> We say that the merchant rate is <strong>favorable</strong> iff the earnings are non-negative for <em>most</em> sets of <em>typical</em> purchases and sales.
<code>r'(t) is favorable iff e(P, S) &gt;= 0</code>.</p>

<p>In a favorable case, the merchant didn&rsquo;t lose any fiat by accepting crypto.</p>

<p><em>most</em> and <em>typical</em> will not be rigorously defined.</p>

<p>As part of <em>typical</em>, we can assume that merchants will sell their crypto in a timely manner.
So assume <code>s_i(2) - s_j(2) &lt; W</code> for <code>i,j in {1.. M}</code> for some bound <code>W</code>.
Purchase amounts should be randomly distributed within a reasonable range that commerce is done. Perhaps $10-100.</p>

<p><strong>The goal of the bot is to verify that <code>r'(t)</code> is favorable.</strong></p>

<p>Note that this definition is only one measure of quality.
Perhaps protecting against the worst case is more important than being favorable.
In that case, we would be concerned about the ability to construct a set of purchases with very negative earnings.</p>

<h3>Algorithm</h3>

<p>Repeat many times:</p>

<ol>
<li>Randomly choose a time range <code>[t0, t1]</code>.</li>
<li><p>Generate a set of <strong>purchases</strong> at random times within <code>[t0, t1]</code>.
The price should fall within a range <code>[p0, p1</code>] of <em>typical</em> prices.</p></li>
<li><p>Generate a set of <strong>sales</strong> at evenly spaced times (perhaps with slight random noise) within <code>[t0, t1]</code>.
Each sale should be for the full <strong>balance</strong> at that time.</p></li>
<li>Calculate the <strong>earnings</strong> for these sets.</li>
<li>Record the earnings.</li>
</ol>


<p>After:</p>

<ol>
<li>Report how many earnings were negative and non-negative. Show a percentage for each.</li>
<li>Identify the minimum and maximum earnings and report them.</li>
</ol>


<h3>Conclusion</h3>

<p>As you read this example, I think your tendency may be to think that its statements are obvious.
Certainly, none of these steps are hard.
However, it was surprising to me how many of my assumptions were corrected and how difficult it was to choose an objective definition of a <strong>favorable</strong> outcome.
This process helped me become aware of assumptions I would not have even considered if I had started by simply writing code.
Perhaps the greatest benefit was that after writing it, I was able to quickly review it with a co-worker and make corrections which were easy on paper, but would have been difficult to change in code.</p>

<p>I hope that thinking in the language of math will bring similar benefits to your projects!
Note that this example is only one style of utilizing mathematical thinking.</p>

 ]]></description>
</item>
<item>
<pubDate>Sun, 12 May 2019 00:00:00 </pubDate>
<title>Notes On Vector Libraries</title>
<guid isPermaLink="true">https://justinmeiners.github.io/vector-libs/</guid>
<description><![CDATA[ 
<h1>Notes on Vector Libraries</h1>

<p><strong>05/12/2019</strong></p>

<p>A good vector math library is essential for graphics
and simulation programming.
However, implementing one that is flexible, efficient,
and easy to use is difficult.
Due to so many choices, experienced
programmers tend to write their own to accommodate their preference.</p>

<p>In this article I will survey a few of the most popular techniques and offer some design advice.
I will specifically focus on math theory and C implementations.</p>

<h2>Math</h2>

<p>Before diving into the code.
It&rsquo;s helpful to review some of the math
to understand what we are aiming for. One thing to watch for
is operations that can be defined in terms of each other.
Rarely do I see libraries take advantage of this.</p>

<p><strong>Vector Operations</strong></p>

<p>On vectors in <code>R^N</code></p>

<ul>
<li>addition <code>v + w</code></li>
<li>subtraction <code>v - w</code>. Defined by addition: <code>a - b = a + (-b)</code></li>
<li>multiplication <code>v * w</code></li>
<li>scaling <code>a * v</code></li>
<li>normalization. Defined by length and scaling: <code>1/|v| * v</code></li>
</ul>


<p>From <code>R^N -&gt; R</code></p>

<ul>
<li>dot product <code>&lt;v, w&gt;</code></li>
<li>length <code>|v|</code>. Defined by dot product <code>sqrt(&lt;v,v&gt;)</code></li>
<li>angle. Defined by dot product and length <code>acos(&lt;a,b&gt;/|a||b|)</code></li>
</ul>


<p>Only on specific dimension, such as <code>R^2</code> or <code>R^3</code></p>

<ul>
<li>cross product <code>a X b</code></li>
<li>angle (in the plane)</li>
</ul>


<p><strong>Matrix Operations</strong></p>

<p>On all matrices <code>M(n x m)</code></p>

<ul>
<li>addition <code>A + B</code></li>
<li>subtraction <code>A - B</code>
Defined by addition <code>A - B = A + (-B)</code></li>
<li>scaling <code>bA</code></li>
<li>multiplication <code>AB</code></li>
</ul>


<p>On square matrices <code>M(n x n)</code></p>

<ul>
<li>determinant <code>det(A)</code></li>
<li>inverse <code>A^-1</code></li>
</ul>


<p>Between vectors and matrices</p>

<ul>
<li>multiplication <code>Av</code></li>
</ul>


<p>Most programs use 2, 3, and 4
element vectors, and only a few operations
are specific to a given dimension.
So a lot of code can be condensed by writing algorithms
on N dimensional vectors.</p>

<p>Matrix operations are also very general.
But a few should be kept to a specific
dimension (usually 3x3 or 4x4).
You do not want to implement a
general inverse or determinant function.</p>

<h2>1. Simple Structs</h2>

<pre><code>typedef struct
{
    float x, y, z;
} vec3;

vec3 vec3_add(vec3 a, b)
{
    vec3 r;
    r.x = a.x + b.x;
    r.y = a.y + b.y;
    r.z = a.z + b.z;
    return r;  
}
</code></pre>

<p>This works well for smaller programs.
The best part is that expressions look nice (<code>a + 2.0*(b-d)</code>):</p>

<pre><code>vec3_add(a, vec3_scale(2.0, vec_sub(b, c)));
</code></pre>

<p>But, we have to copy this definition for every dimension.
We also have to avoid any algorithms that use
index or iteration.
Matrix vector multiplication gets ugly.</p>

<p>If you only have a few functions that need
indexing and you can index into a pointer to the first member:</p>

<pre><code>vec3 a;
float* v = &amp;a.x;
v[0];
</code></pre>

<p><strong>Examples</strong></p>

<ul>
<li><p><a href="https://github.com/justinmeiners/pre-rendered-backgrounds/blob/master/source/engine/core/vec_math.h">vec math</a></p></li>
<li><p><a href="https://github.com/nothings/obbg/blob/master/src/stb_vec.h">stb vec</a></p></li>
</ul>


<h2>2. Arrays</h2>

<p>For <code>N</code> dimensional vectors
we might try to write functions which
operate on arrays of floats.
This is nice because it does not
introduce another data structure, so
other functions and vector libraries play nice with each other.</p>

<p>Unfortunately, C does not allow you to return
an array from the stack. You can only return a pointer
which must point to some valid region.
So either we do something horrible like <code>malloc</code> in each operation,
or pass in arrays for the return value.
Passing in arrays works, but it destroys the ability
to comfortably write simple expressions such as <code>a + 2.0*(b-d)</code>:</p>

<pre><code>void vecn_add(int n, float* a, float* b, float* ret);

// intermediate results everywhere
float temp[3];
vecn_sub(3, b, d, temp);

float temp2[3];
vecn_scale(3, 2.0, temp, temp2);

float final[3];
vecn_add(3, a, temp2, final);
</code></pre>

<p>Plain arrays may be appropriate for matrices since they are not typically
involved in complex expressions.
Matrices and large vectors, which would be inefficient to copy around
would also be a good use case.</p>

<p>Depending on the application you may not want
to sacrifice performance by introducing loops and branching
into every operation. As long as the dimensions
are input as a literals or macros, the small loops
should be unrolled at compile time.</p>

<p><strong>Examples:</strong></p>

<ul>
<li><a href="https://developer.apple.com/documentation/accelerate/vdsp">Accelerate</a></li>
<li><a href="https://github.com/datenwolf/linmath.h">linmath</a></li>
</ul>


<h2>3. Struct + Union</h2>

<p>A workaround to return an array from a function is to include it in a struct.
The tradeoff is that the size must be fixed and element access is a bit
uglier as it requires at least an extra letter.</p>

<pre><code>typedef struct
{
    float e[3];
} vec3;

vec3 v;
v.e[0] = 1;
</code></pre>

<p>The access syntax can be cleaned up with a union
but, <a href="https://gcc.gnu.org/onlinedocs/gcc/Unnamed-Fields.html">anonymous structs/unions</a>
are a GCC extension and are non-standard.</p>

<pre><code>typedef union
{
    float v[3];
    struct
    {
        float x;
        float y;
        float z; 
    }; 
} vec3;
</code></pre>

<p>This gives you safe iterative access and nice named members,
but it is hard to combine with generic functions.
Either you use functions which operate on the internal arrays
and deal with the intermediate results.
Or, define fixed dimension functions which wrap
the generic ones:</p>

<pre><code>vec3 vec3_add(vec3 a, vec3 b);
{
    vec3 temp;
    vecn_add(3, a.v, b.v, temp.v);
    return temp;
}
</code></pre>

<p>I don&rsquo;t love this option.
If I need to write wrapper functions I might as well go back to method 1
and copy implementations around.</p>

<p><strong>Examples:</strong></p>

<ul>
<li><a href="https://github.com/arkanis/single-header-file-c-libs/blob/master/math_3d.h">Math 3D</a> (See Matrices)</li>
</ul>


<h2>4. Macros</h2>

<p>Some clever macros can help you get the best of both worlds, and
parameterize the scalar types. This can be combined with
an array or union data structure.
But, writing multi-line macros isn&rsquo;t very fun.</p>

<pre><code>#define DEFINE_VEC(T, N, SUF) \
\
void vec##N####SUF##_add(const T *a, const T *b,  T *r) \
{ \
    for (int i = 0; i &lt; N; ++i) \
        r[i] = a[i] + b[i]; \
} \
</code></pre>

<p>Then define the types you need:</p>

<pre><code>DEFINE_VEC(float, 2, f);
DEFINE_VEC(float, 3, f);
DEFINE_VEC(float, 4, f);
</code></pre>

<p>Usage:</p>

<pre><code>vec3f_add(a, b);
</code></pre>

<p>Functions which only apply to a specific dimension
can be defined outside of the macro:</p>

<pre><code>void vec3f_cross(const float* a, const float* b, float* r)
{
    // ...
}
</code></pre>

<p><strong>Examples:</strong></p>

<ul>
<li><a href="https://github.com/datenwolf/linmath.h">linmath</a></li>
</ul>


<h2>Closing Thoughts</h2>

<p>In typical C fashion, I believe it is misguided to try to write
the <em>one true</em> vector library to serve all purposes.
These libraries are bloated and must choose tradeoffs which
don&rsquo;t fit your use case. Instead use the examples
above to write to tailor make vector functions as needed.</p>

<p>For further reading, see <a href="http://www.reedbeta.com/blog/2013/12/28/on-vector-math-libraries/">On Vector Math Libraries</a>.
It focuses on C++ and has a few other handy tips.
You can also read a <a href="https://github.com/arkanis/single-header-file-c-libs/issues/3">discussion</a>
which led to these notes.</p>

 ]]></description>
</item>
<item>
<pubDate>Sun, 14 Apr 2019 00:00:00 </pubDate>
<title>Foundations of Math Reading List</title>
<guid isPermaLink="true">https://justinmeiners.github.io/foundations-of-math-reading/</guid>
<description><![CDATA[ 
<h2>Foundation of Math Reading List</h2>

<p><strong>04/13/2019</strong></p>

<p>One of my favorite courses in college was
philosophy of language.
Along with interesting philosphy
it introduced me to the foundations of math project
which has since become one of my
favorite subjects to learn about.</p>

<p>Some of my favorite books of all time come out of this interest.
I wanted to organize a few of these
into a list for others who are interested in the topic.
Note that many good books were left out in favor of the very best.</p>

<p>You will notice a theme in my commentary.
The books I like most are those that don&rsquo;t
shy away from hard technical knowledge, but also
explore the philosophical ideas behind them.</p>

<p>The list is arranged in a progressive sequence
that will help prepare you for the next one.</p>

<h3>Logicomix</h3>

<p>By: <a href="https://www.apostolosdoxiadis.com">Apostolos Doxiadis</a></p>

<p><img src="logicomix.jpg" alt="logicomix" /></p>

<p>Logicomix is actually a comic book!
It tells an engaging historical narrative about the search for the foundations of math
and the birth of analytic philosophy, in the early 20th century.</p>

<p>It introduces you to all the major characters
such as Gdel, Russel, Frege, and Wittgenstein and
motivates the kinds of problems they were trying to solve.
The book also explores how these ideas connect to modern computer science.</p>

<p>It is an absolute joy to read and will give you a taste
of whether this is an interesting subject for you.</p>

<h3>Gdel, Escher, Bach</h3>

<p>By: <a href="https://en.wikipedia.org/wiki/Douglas_Hofstadter">Douglas Hofstadter</a></p>

<p><img src="geb.jpg" alt="godel escher bach" /></p>

<p>You probably have seen this work recommended elsewhere.
Gdel, Escher, Bach really deserves all the praise that it gets.</p>

<p>Hofstadter covers an enormous range of topics
including formal systems, Godel&rsquo;s proof, theory of computation,
programming, molecular biology, and artificial intelligence.
Every topic is presented beautifully and with a lot of philosophical discussion.
In many ways it is an introduction to the big ideas in modern science.
It is written for a general audience and assumes no mathematical background.</p>

<h3>Gdel&rsquo;s Proof.</h3>

<p>By: <a href="https://en.wikipedia.org/wiki/Ernest_Nagel)">James Newman</a> &amp; <a href="https://en.wikipedia.org/wiki/James_R._Newman">Ernest Nagel</a></p>

<p><img src="godels_proof.jpg" alt="godel's proof" /></p>

<p>Gdel, Escher, Bach does a good job of introducing the incompleteness theorem
and discussing its ramifications, but if you are like me you probably still won&rsquo;t
completely understand it after a first reading.</p>

<p>This concise book offers another perspective and a clear explanation
of the mathematics of the proof, its general strategy, and the
historical context surrounding the incompleteness problem.</p>

<h3>Descartes Dream</h3>

<p>By: <a href="https://en.wikipedia.org/wiki/Philip_J._Davis">Phillip Davis</a> &amp; <a href="https://en.wikipedia.org/wiki/Reuben_Hersh">Reuben Hersch</a></p>

<p><img src="descartes_dream.jpg" alt="descartes dream" /></p>

<p>In the 17th century Descartes had a dream in which he saw
a future world driven by mathematical calculations and logical systems.
The theme of this book is how this dream has become a reality.</p>

<p>The book explores how mathematics and computer science work together,
surveys several interesting fields, and examines ethical issues in the technological world.
This book is not technical, and should be appropriate for anyone interested in science
or technology.</p>

<p>If you like this book, the authors wrote another called <em>The Mathematical Experience</em>
which is focused more on pure mathematics.</p>

<h3>Computation: Finite &amp; Infinite</h3>

<p>By: <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a></p>

<p><img src="computation.jpg" alt="computation finite &amp; infinite" /></p>

<p>Marvin Minsky is an incredibly clear and deep writer.
In this work he provides a mathematical framework
for thinking about mechanical machines and develops the
theory of computation.</p>

<p>This book teaches you all you need to know about Turing machines, finite state, and neural networks.
My project: <a href="https://justinmeiners.github.io/neural-nets-sim/">McCulloch &amp; Pitts Neural Net Simulator</a>
is based on this book.</p>

<p>To read this book, you should understand some logic and basic set theory
such as that taught in an introductory proofs course.</p>

<p>Unfortunately, it is out of print and may be difficult to obtain (for a reasonable
amount of money).
I read it from my university&rsquo;s library. If that is not an option, I recommend you <em>find it online</em>.
If anyone knows of a place where I can reasonably purchase this book, let me know.</p>

<h3>Structure and Interpretation of Computer Programs</h3>

<p>By: <a href="https://en.wikipedia.org/wiki/Gerald_Jay_Sussman">Gerald Sussman</a> &amp; <a href="https://en.wikipedia.org/wiki/Hal_Abelson">Hal Abelson</a></p>

<p><img src="sicp.jpg" alt="sicp" /></p>

<p><a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html)">Read Online</a></p>

<p>This classic text is designed to teach programming to MIT students
who have some technical background in another areas of math and science.
It is a hard read, but it assumes no programming knowledge and teaches Scheme (a Lisp dialect) and
its full inter-workings from the ground up.</p>

<p>If you want to be a professional programmer,
this may be the only book you need to study.
What other book teaches you to write a symbolic differentiator,
interpreter, circuit simulator, and compiler?</p>

<p>Most of the material is mixed in the <a href="https://github.com/justinmeiners/excercises/tree/master/sicp">excercises</a>
so don&rsquo;t skip them!</p>

<p>But, this is not just a programming book.
It belongs in this list because it teaches the fundamental concepts of computation.
See the section <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1.5">data as programs</a> for an example.</p>

<h3>Logical Foundations of Mathematics and Computational Complexity</h3>

<p>By: <a href="http://users.math.cas.cz/~pudlak/">Pavel Pudlak</a></p>

<p><img src="logical_foundations.jpg" alt="logical foundations of math" /></p>

<p>This book is a massive and dense survey of topics including
formal systems, set theory, abstract algebra, computability theory,
analysis of algorithms, and quantum computing.
The first chapter covered almost everything I had learned about
algebra and meta-mathematics in my entire undergraduate degree!</p>

<p>Pudlak does a fantastic job of balancing technical
information with philosophical discussion.
I can&rsquo;t recommend this book enough.</p>

<p>Reading this book definitely requires some mathematical maturity.
The author does his best to explains every concept in the book
but it would be hard for me to read about a &ldquo;group&rdquo; for the first time
and really understand what he means.</p>

<p>If you read through the other books, and have technical knowledge
you should be well prepared.</p>

 ]]></description>
</item>
<item>
<pubDate>Fri, 22 Feb 2019 00:00:00 </pubDate>
<title>The Skills Poor Programmers Lack</title>
<guid isPermaLink="true">https://justinmeiners.github.io/the-skills-programmers-lack/</guid>
<description><![CDATA[ 
<h2>The Skills Poor Programmers Lack</h2>

<p><strong>02/22/2019</strong></p>

<p>Updated: <strong>04/27/2020</strong></p>

<p>A friend and I had a discussion about the basic skills that are often lacking in experienced programmers. How can a programmer work for ten or twenty years and never learn to write good code? Too often they need close supervision to ensure they go down the right path, and can never be trusted to take technical leadership on larger tasks. It seems they are just good enough to get by in their job, but they never become <em>effective</em>.</p>

<p>We thought about our experiences and came up with three fundamental skills that we find are most often missing. Note that these are not skills which take a considerable amount of talent or unique insight. Nor are they &ldquo;trends&rdquo; or &ldquo;frameworks&rdquo; to help you get a new job. They are basic fundamentals which are prerequisites to being a successful programmer.</p>

<h2>Understand how the language works</h2>

<p>Programmers cannot write good code unless they understand what they are typing. At the most basic level, this means they need to understand the rules of their programming language well. It is obvious when a programmer doesn&rsquo;t because they solve problems in indirect ways and litter the code with unnecessary statements that they are clueless as to what they actually do. Their mental model of the program does not match with the actual behavior of the code.</p>

<p>You may have seen code which misunderstands how expressions work: (1)</p>

<pre><code>if isDelivered and isNotified:
    isDone = True
else:
    isDone = false;
</code></pre>

<p>Instead of:</p>

<pre><code>isDone = isDelivered and isNotified
</code></pre>

<p>In JavaScript, this is often indicated by <code>new Promise</code> inside a <code>.then()</code>. In C++, it is attaching <code>virtual</code> to every method and destructor and creating every object with <code>new</code>.</p>

<p>Debugging is also extremely difficult if you don&rsquo;t understand the language. You may add a line of code because it fixes a bug for reasons you don&rsquo;t understand. Bugs are mysteries that seem to appear organically, like dust on the shelves. The code has a mind of its own.</p>

<p>Understand the code you write. Know what every line does and why you put it there.</p>

<p>Once you understand the language well, its important to know about implementation; what goes on <em>inside</em> the computer or library? Do you know how the code gets to <a href="http://inst.eecs.berkeley.edu/~cs61c/sp15/">assembly</a>? Do you know how a <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-21.html#%_sec_3.2">closure captures variables</a>? Do you know how the <a href="https://en.wikipedia.org/wiki/Cheney%27s_algorithm">garbage collector</a> works? Do you know how a <a href="http://opendatastructures.org/ods-python/">map/dictionary</a> works? Do you know how an HTTP request is made?</p>

<p>Modern software and computers are too complex to know everything (2).
The idea is not not to be too clever, but to avoid doing silly things.
Silly mistakes result in sluggish code that is wasteful of system resources, or just does unexpected things.
Removing an item from the front of a C++ <code>vector</code> requires copying the entire vector which is worth thinking about in large cases.
Writing <code>map&lt;string, map&lt;string, ...&gt;&gt;</code> in C++ (a dictionary of dictionaries) creates a a self-balancing tree in which every node is a self-balancing tree, a data structure nobody would intentionally design. (3)</p>

<p>A muddy understanding of how things work is typical of beginners, but it is all too often a problem with experienced programmers if they are not curious and do not take time to learn how things work beyond their immediate jobs needs.
Learn just a bit more about the stuff you use most works.</p>

<h2>Anticipate problems</h2>

<p>To write reliable code, you must be able to anticipate problems, not just patch individual use cases. I am shocked by the number of times I see code that puts the program in a broken state when a very likely error happens.</p>

<p>I recently reviewed some code that made an HTTP request to notify a server of a state change in which the programmer assumed the HTTP request would always succeed. If it failed, (and we know how often HTTP requests fail), a database record was put into an invalid state. The questions they should have asked when writing this code are: What happens if this fails? Is there another opportunity to send the notification? When is the correct time to record the state change? Careful programmers think through the possible states and transitions of their program.</p>

<p>Using <code>sleep()</code>, cron jobs, or <code>setTimeout</code> is almost always wrong because it typically means you are waiting for a task to finish and dont know how long it will take. What if it takes longer than you expect? What if the scheduler gives resources to another program? Will that break your program? It may take a little bit of effort to rig a proper event, but it is always worth it.</p>

<p>Another common mistake I see is generating a random file or identifier and hoping that collisions will never happen (4). It is reasonable for an unlikely event to cause an error, but it is not ok if that puts your program in an unusable state. For example, if a successful login generates a session token and it collides with another token, you could reject the login and have the user try again. It is a freak accident that slightly inconveniences the user. On the other hand, what if you generate storage files with random names and you have a collision? You just lost someones data! This probably wont happen is not a strategy for writing reliable code.</p>

<p>Unit testing cant solve this problem either. It can help you stop and think about some inputs to write a test, but more than likely, the cases that you write tests are the ones you anticipated when you wrote the code! Unit testing cannot transform fragile code into reliable code.</p>

<p>Fragile code is often caused by a lack of experience regarding things that can go wrong, but it can also be the result of a long career of maintaining existing codebases. When working on a large existing system, you typically fix individual bugs and arent rewarded by your bosses for improving the system as a whole. You learn that programming is a never-ending patch. Increasing the <code>sleep()</code> time may fix the bug today, but never solve the underlying issue.</p>

<h2>Organize and design systems</h2>

<p>Even when armed with the other two skills, it&rsquo;s hard to be effective unless you can organize code into a system that makes sense. I believe OOP and relational database get a lot of flack because programmers tend to be bad at design, not because they are broken paradigms. You simply can&rsquo;t create rigid classes, schemas, and hierarchies without thinking them through. Design itself is too broad a topic to explore in this article (read <a href="https://en.wikipedia.org/wiki/Fred_Brooks">Fred Brooks</a>), so I want to focus on a few specific attributes that well-designed software tends to have.</p>

<p>You may have heard a rule like <a href="http://number-none.com/blow/blog/programming/2014/09/26/carmack-on-inlined-code.html">&ldquo;don&rsquo;t make functions or classes too long&rdquo;</a>. However, the real problem is writing code that mixes unrelated ideas. Poorly designed software lacks conceptual integrity. Its concepts and division of responsibilities are not well defined. It usually looks like a giant Rube Goldberg machine that haphazardly sets state and triggers events.</p>

<p>Accordingly, good software is built from well-defined concepts with clear responsibilities. Mathematicians and <a href="https://en.wikipedia.org/wiki/Categories_(Aristotle)">philosophers</a> spend a lot of time discussing definitions because a good definition allows them to capture and understand some truth about the world. Programmers should think similarly and spend a comparable amount of effort grappling with ideas before writing code.</p>

<p>Good programmers ask questions like:</p>

<ul>
<li>&ldquo;What is this function&rsquo;s purpose?&rdquo;</li>
<li>&ldquo;What does this data structure represent?&rdquo;</li>
<li>&ldquo;Does this function actually represent two separate tasks?&rdquo;</li>
<li>&ldquo;What is the responsibility of this portion of code? What shouldn&rsquo;t it &lsquo;know about&rsquo;?&rdquo;</li>
<li>&ldquo;What is necessary to be in the public interface?&rdquo;</li>
</ul>


<p>Luckily the field is ripe with strategies to help you design code. <a href="https://www.oodesign.com">Design patterns</a> and <a href="https://en.wikipedia.org/wiki/SOLID">SOLID</a> can give you guidelines for designing classes. Functional programming encourages writing pure functions (input -> output and no side effects) and maintaining as little state as possible. <a href="https://developer.apple.com/library/archive/documentation/General/Conceptual/DevPedia-CocoaCore/MVC.html">Model view controller</a> aims to separate UI and storage concerns from program logic. On the other hand, React components form conceptual units by combining the HTML, CSS, and JS into a single component. Unix rejects categories and says <a href="https://en.wikipedia.org/wiki/Everything_is_a_file">everything is a file</a>. All of these seemingly contradictory ideas are valid. The important thing is that the concepts make sense and map closely to the problem you are solving.</p>

<p>Software that is well-designed is also software that is easy to change. Of course, it&rsquo;s too much to ask it to satisfy requirements that contradict its original intent. But, it should accommodate changes that are natural evolutions. A common mistake I see is solving a problem for a few cases, instead of N cases. (If you have a variable called <code>button3</code>.) Another is treating everything as a special case using <code>switch</code> statements instead of using polymorphism. (5)</p>

<p>I think the best way to learn about design is to write and study a lot of programs. Programmers who work only on old programs never learn to write new ones. The studying part is key too. Programmers who only work on small temporary projects (like an agency) may get by without ever improving how to design programs. Good design comes gradually with experience, but only if you think about it and try to improve.</p>

<p>There are no tricks or rules that you can follow to guarantee you will write good software. As Alex Stepanov said, &ldquo;think, and then the code will be good.&rdquo;</p>

<ol>
<li><p>There may be some cases where the previous style is preferred.
This example is only an illustration.</p></li>
<li><p>I was shocked when I wrote some multithreaded code and first faced bugs due to
<a href="https://en.wikipedia.org/wiki/Cache_coherence">Cache coherence</a> and <a href="https://preshing.com/20120625/memory-ordering-at-compile-time/">instruction reordering</a>!</p></li>
<li><p>A reader corrected me about the performance of modern <code>map</code> implementations.
Map of map is just as good as alternatives, like map of pair.
I think it is a good illustration of code being suprising,
but perhaps I am doing a bit too much early optimization :)
See his <a href="https://gist.github.com/Dobiasd/fa27e3efb8b08fc81791d7f8e51ac5ca">benchmark</a>.</p></li>
<li><p>There are ways to do this sort of thing correctly using cryptographic hashes.</p></li>
<li><p>Once again, this is just an example. Switching on type may be perfectly appropriate.</p></li>
</ol>


 ]]></description>
</item>
<item>
<pubDate>Sun, 13 Jan 2019 00:00:00 </pubDate>
<title>McCulloch &amp; Pitts Neural Net Simulator</title>
<guid isPermaLink="true">https://justinmeiners.github.io/neural-nets-sim/</guid>
</item>
<item>
<pubDate>Sat, 05 Jan 2019 00:00:00 </pubDate>
<title>An Adventure in Pre-Rendered Backgrounds</title>
<guid isPermaLink="true">https://justinmeiners.github.io/pre-rendered-backgrounds/</guid>
</item>
<item>
<pubDate>Sun, 16 Dec 2018 00:00:00 </pubDate>
<title>Write your Own Virtual Machine</title>
<guid isPermaLink="true">https://justinmeiners.github.io/lc3-vm/</guid>
</item>
<item>
<pubDate>Thu, 19 Apr 2018 00:00:00 </pubDate>
<title>Lisp Interpreter</title>
<guid isPermaLink="true">https://github.com/justinmeiners/lisp-interpreter</guid>
</item>
<item>
<pubDate>Sat, 27 Jan 2018 00:00:00 </pubDate>
<title>Spherical Harmonics</title>
<guid isPermaLink="true">https://github.com/justinmeiners/spherical-harmonics</guid>
</item>
<item>
<pubDate>Thu, 01 Jun 2017 00:00:00 </pubDate>
<title>Shamans: A 3D Turn-based Strategy Game for iPad</title>
<guid isPermaLink="true">https://justinmeiners.github.io/shamans/</guid>
</item>
<item>
<pubDate>Mon, 01 May 2017 00:00:00 </pubDate>
<title>Modern OpenGL</title>
<guid isPermaLink="true">https://justinmeiners.github.io/modern-opengl/</guid>
<description><![CDATA[ 
<h2>Modern OpenGL</h2>

<p>These are notes sent to a friend, not a formal article.
I thought they might be useful for others as well.</p>

<p><strong>05/01/2017</strong></p>

<p>If you&rsquo;re looking for an easy way to do 3D its probably <a href="https://developer.apple.com/documentation/scenekit">SceneKit</a> or Unity,
but I assume you want to learn how things work.</p>

<p>Everything you know and love in OpenGL is gone. <code>glTranslate</code>, <code>glLookAt</code>, <code>glRotate</code>, <code>glVertex</code>, <code>glLight</code>, etc.
The reasoning is that these all describe a specific way of doing things, and OpenGL wanted to be as general as possible so that you can make whatever you want.</p>

<p>For example <code>glLight</code> has a built in algorithm for lighting. What if you want to make your own? You cant.</p>

<p>The solution is to make everything programmable. Instead of having fixed GPU procedures, the GPU becomes flexible so you can write code which runs on the GPU that replaces the fixed procedures.
These programs are called shaders.</p>

<h2>Shaders</h2>

<p>There are two kinds of shaders you need to worry about.</p>

<p><strong>Vertex Shader:</strong> takes input data, outputs vertex information (position, color, etc)</p>

<p>The outputs from this vertex shader are interpolated across the triangle (or line) you are drawing, and the results are inputed into the fragment shader.</p>

<p><strong>Fragment Shader:</strong> takes input data, and interpolated vertex information, and outputs a color.</p>

<p>Shaders are written in a C like language and compiled into native GPU code.
I would say the first thing you need to do is practice writing some shaders, so you can get an idea of what data you need to give them.</p>

<p>The modern OpenGL API has been totally stripped down. It basically consists of creating shaders, providing them with data, and executing them.
That is it. There is nothing else in OpenGL.</p>

<h2>Data</h2>

<p>So here is the kind of data you need to prepare for OpenGL. This is done in your main code.</p>

<h3>vertex positions, vertex colors, vertex normals.</h3>

<p>You use vertex buffers to upload this data to the GPU. Basically you can make a vertex struct:</p>

<p><code>
struct vertex
{
    Vec3 position, normal, color
}
</code></p>

<p>and  then describe its structure, and then upload an array of them. You could make one vertex buffer for your sphere and then reuse it for each ball.</p>

<h3>Textures</h3>

<p>You upload images using <code>glTexImage</code>. This APIs has stayed mostly the same as old OpenGL.</p>

<h3>Matrices</h3>

<p>Since OpenGL has no translate, rotate, etc. You need to create your own transformation matrices for every object in the scene.</p>

<p>This has a few components:</p>

<ul>
<li><strong>model matrix</strong> - this is the translation/rotation for each individual model.</li>
<li><strong>view matrix</strong> - this describes the camera in the scene (<code>gluLookAt</code>)</li>
<li><strong>projection matrix</strong> - this is the 3D lense which projects 3D coordinates onto the 2D screen.</li>
</ul>


<p>Basically you have to calculate all of these yourself, pass them to your vertex shader, and then use them to translate the vertex positions.</p>

<p>Usually the projection and matrices are constant for one draw of the scene, you create them at the beginning and pass them with everything.
Then for each model you need to construct a new model matrix, describing its current state.</p>

<h3>Recommended Resources</h3>

<ul>
<li><a href="https://learnopengl.com/">Learn OpenGL</a></li>
<li><a href="https://paroj.github.io/gltut/">Learning Modern 3D Graphics Programming</a></li>
</ul>


 ]]></description>
</item>
<item>
<pubDate>Thu, 10 Nov 2016 00:00:00 </pubDate>
<title>HoloLens Impressions</title>
<guid isPermaLink="true">https://justinmeiners.github.io/hololens-impressions/</guid>
<description><![CDATA[ 
<h2>HoloLens First Impressions</h2>

<p><strong>11/10/2016</strong></p>

<p><img src="hl_2.jpg" alt="HoloLens" /></p>

<p>At work, I recently started on a new medical application for the <a href="https://www.microsoft.com/microsoft-hololens/en-us">Microsoft HoloLens</a>. I don&rsquo;t follow popular technology news, and rarely get excited about new gadgets, but I feel that the HoloLens is a robust platform, with real substance beyond the usual VR gimmicks. In this article I will talk a little bit about my experience with the hardware and development environment, as well as outline some its shortcomings. As the current release is a &ldquo;Developer Edition&rdquo; I hope some of the items will be addressed in the consumer release.</p>

<p>Where the HoloLens succeeds most, is in offering an entirely distinct experience from VR headsets such as the Vive and Occulus. Everyone I have spoken to, who has tried both, immediately agrees the two are like apples and oranges. There is no question that the HTC Vive offers the most immersive graphical experience, but I get the feeling I could actually use the HoloLens for work, education, and other tasks beyond entertainment. The crisp display, is clear and readable, and since it doesn&rsquo;t overwhelm your vision it doesn&rsquo;t feel nauseating. The likelihood, of me setting up a full Vive room setup, to look at a piece of anatomy or the solar system is small, but quickly putting on the HoloLens to view a models from a true 3D perspective is helpful.</p>

<p>Microsoft recommends using Unity to get started with HoloLens development, while also providing native C++ and Direct 3D. Initially. using Unity didn&rsquo;t excite me, but the it ends up working fairly well. Unity has a specific version of its engine adapted for HoloLens. This includes APIs and components for accessing the room scanning, world anchors, gestures, and voice controls. Once a build is exported from Unity, you can run, debug, and write code all in Visual Studio, until you need to modify the scene. The HoloLens can connect directly through USB or the WiFi for debugging. Microsoft has solid <a href="https://developer.microsoft.com/en-us/windows/holographic/documentation">documentation</a> and tutorials to help you figure everything out.</p>

<p>The voice controls are excellent, and easy to develop for. While in my quiet office, I have never had it mistake me for simple commands such as &ldquo;back&rdquo; and &ldquo;select&rdquo;. I&rsquo;ll even admit to working with food stuffed in my mouth, and it still recognized my commands about half the time. Speaking commands to yourself is still as embarrassing as it sounds (less so than shouting at your phone), but the technology is solid, and I believe there is potential here. Development for voice controls is a pleasant process as well. In Unity you can simply register phrases you would like to listen for, and receive callbacks when they are spoken.</p>

<p>The most noticeable limitation is the small field of view. Even though the glasses surround almost your whole vision, the displayable portion takes up only a small rectangular box at the front of your vision. This means that any objects larger than a basketball get chopped off while standing only a few feet away. This ugliness is most noticeable when a large model fills the whole viewing rectangle, leaving everything outside blank. It looks as if you are looking at the world through an open cereal box. Unfortunately, I can&rsquo;t show you what this looks like in a screenshot.</p>

<p><img src="cutoff_screen.jpg" alt="viewport" /></p>

<p>The gesture controls aren&rsquo;t terrible, but still feel clunky. It takes a little bit of time to master, and then it&rsquo;s sort of like having a mouse that fails to click every tenth time. They are made worse when other people are nearby to distract the sensor.</p>

<p>Since the entire system is contained within the headset, its GPU is more comparable to a mobile phone than a desktop machine. While not weak, the hardware feels a bit under powered. The frame rate cuts to about 30 frames, with roughly 50k triangles across 25 draw calls. I don&rsquo;t expect a ton here, but a bit faster would be nice. In my experience an iPad 2 has better GPU performance, but probably does a lot less background work. Given that I only tested in Unity, it is possible that using native C++ and Direct X would be a dramatic improvement. If you are using Unity be sure to use the optimized shaders found in <a href="https://github.com/Microsoft/HoloToolkit-Unity">Microsoft&rsquo;s Holokit</a>, and be conscious of batches.</p>

<p>The shortcomings above can hopefully be addressed in future iterations of the hardware. In its current form, it is still a fundamentally innovative platform and I am excited to continue developing for it.</p>

 ]]></description>
</item>
<item>
<pubDate>Sun, 30 Oct 2016 00:00:00 </pubDate>
<title>3D Paint</title>
<guid isPermaLink="true">https://justinmeiners.github.io/3d-paint/</guid>
<description><![CDATA[ 
<h2>3D Paint </h2>

<p><strong>10/30/16</strong></p>

<p>In 2012 I came across the paper <a href="http://zurich.disneyresearch.com/OverCoat/">OverCoat: An Implicit Canvas for 3D Painting</a>
which describes a technique for creating 3D models using traditional painting methods.
At the time I didn&rsquo;t have sufficient mathematical knowledge
to understand all of the technical details described, but I picked up the general idea and wrote my own rough implementation.
This article explains how my program was constructed and some of what I learned in the process.</p>

<p><img src="anim.gif" alt="3d paint gif" /></p>

<p>The basic idea is to paint 3d strokes onto a rough 3D model which acts as a canvas.
The model is simply rotated to a desired orientation, and drawn upon, as if one was painting ontop of a flat a picture.
The difference is that the strokes are projected into 3D space giving them their own 3D form.</p>

<p>The model is only a placeholder which is eventually removed, leaving only the painting made up of strokes.
To position the nodes which define a stroke, my program simply performed a ray-triangle test between the mouse ray and the mesh, and offset the intersection point by the face normal.
The paper describes a sophisticated optimization method which evenly distributes stroke nodes, even across harshes changes in depth.</p>

<p><img src="screen1.jpg" alt="screen 1" /></p>

<p>The image above shows several strokes projected onto the surface of the model.
Along with the node positions, information such as a color and brush size are stored for each stroke.</p>

<p>I used the following data structures:</p>

<pre><code>typedef struct
{
    Vec4_t color;
    int brush;
    float pressure;
} StrokeInfo_t;

typedef struct
{
    Vec3_t pos;
    float radius;
} StrokeNode3D_t;

typedef struct
{
    int nodeCount;
    StrokeNode3D_t nodes[MAX_STROKE_NODES];
    Vec4_t color;
    // used for depth sorting
    Vec3_t center;
    StrokeInfo_t info;
} Stroke3D_t;

typedef struct
{
    int strokeCount;
    Stroke3D_t strokes[MAX_STROKES];
    int dirty;
} Canvas3D_t;
</code></pre>

<p><img src="screen2.jpg" alt="screenshot 2" /></p>

<p>To render the strokes, each node position is transformed from 3D onto the 2D screen, using the camera matrix. The brush image is then rendered onto the framebuffer at each of the 2D node positions on the screen. Even though these screenshots show only a few colors and a single brush, the program supports multiple brush types and RGB colors.</p>

<p><img src="screen4.jpg" alt="screenshot 4" /> <img src="screen3.jpg" alt="screenshot 3" /></p>

<p>As the drawing becomes more dense, the strokes begin to resemble their own shape.
Rendering strokes is a very efficient process because it only involves stamping the brush image in 2D, which is easily parallelized on the GPU.
For the strokes to be drawn in the correct order they must be sorted by depth. This is by far the most computational expensive part of the process.
My program solved this problem by only sorting strokes by their average position, which seemed to be an acceptable alternative to sorting each node.
The order in which the nodes of a stroke are rendered is still important to ensure proper layering. An offline rendering process should sort each node properly.</p>

<p><img src="screen5.jpg" alt="screenshot 5" /> <img src="screen6.jpg" alt="screenshot 6" /></p>

<p>Although my program cannot produce the beautiful paintings found in the original paper, it can still draw some fun pictures.
I think the technique itself is very interesting and has great potential. It would be interesting to make a small game, or animation centered around the technique since the artwork is so easy and natural to produce.
Because it requires no textures, other than the small brushes, its memory usage is also very small, while its vertex requirements are similar to triangle meshes.
The next natural step would be to bind a painting to a skeleton for animation.</p>

 ]]></description>
</item>
<item>
<pubDate>Tue, 01 Dec 2015 00:00:00 </pubDate>
<title>Molecule Viewer</title>
<guid isPermaLink="true">https://github.com/justinmeiners/molecule-viewer</guid>
</item>
<item>
<pubDate>Thu, 15 Oct 2015 00:00:00 </pubDate>
<title>Old Artwork</title>
<guid isPermaLink="true">https://justinmeiners.github.io/old-artwork/</guid>
<description><![CDATA[ 
<h2>Old Artwork</h2>

<p><strong>10/15/2015</strong></p>

<p>I am not much of a visual artist, but I still enjoy 3D modeling and animation. This page contains pictures of some of my older models I have worked on.</p>

<h3>Moon Rover</h3>

<p><img src="rover1.png" alt="moon rover 1" /> <img src="rover2.png" alt="moon rover 2" /></p>

<h3>Low-poly Cargo Ship</h3>

<p><img src="cargo_ship1.png" alt="cargo ship 1" /> <img src="cargo_ship2.png" alt="cargo ship 2" /></p>

<p><img src="cargo_ship4.png" alt="cargo ship 4" /> <img src="cargo_ship3.png" alt="cargo ship 3" /></p>

<h3>Space Invader Sculpt</h3>

<p>While working at Infuse Medical, I taught a course on 3D programming with OpenGL. As part of my course we created a small space invaders clone. Another developer created the low-poly model for the alien, which I use to create the high resolution sculpt shown below. Occlusion maps were generated from the sculpt for the game.</p>

<p><img src="space_invaders1.png" alt="alien 1" /> <img src="space_invaders2.png" alt="alien 2" />
<img src="space_invaders_ipad1.png" alt="space invaders ipad" /> <img src="space_invaders_ipad2.png" alt="space invaders ipad" /></p>

<h3>Low-poly Radio</h3>

<p><img src="radio1.png" alt="radio 1" /> <img src="radio2.png" alt="radio 2" /> <img src="radio3.png" alt="radio 4" /></p>

<h3>Pixel Art</h3>

<p><em>
<img alt="knight pixel art" src="knight_pixel.png" class="pixel-art"/>
<img alt="alien pixel art" src="alien_pixel.png" class="pixel-art"/>
<img alt="rocket pixel art" src="rocket_pixel.png" class="pixel-art"/>
<img alt="owl pixel art" src="owl.png" class="pixel-art"/>
</em></p>

<h3>Another Ship</h3>

<p><img src="another_ship.png" alt="another ship" /></p>

 ]]></description>
</item>
<item>
<pubDate>Fri, 09 Oct 2015 00:00:00 </pubDate>
<title>Old Games</title>
<guid isPermaLink="true">https://justinmeiners.github.io/old-games/</guid>
<description><![CDATA[ 
<h2>Old Games</h2>

<p><strong>10/09/2015</strong></p>

<p>I spent quite a bit of time experimenting with game technology when I was first learning how to program. I played with tons of ideas, many of which turned out to be no good, but I learned a ton about graphics and development in the process.</p>

<p><img src="minecraft.jpg" alt="minecraft clone screenshot" /></p>

<p>An extremely fast minecraft clone. Written in C and OpenGL.</p>

<p><img src="lightmap.jpg" alt="lightmap screenshot" /></p>

<p>I wrote my own lightmapper and UV unwrapper in Objective-C.</p>

<p><img src="asteroids.jpg" alt="asteroids screenshot" /></p>

<p>A small demo for a sci-fi adventure game engine. Written in C and OpenGL.</p>

<p><img src="ocean.jpg" alt="ocean screenshot" /></p>

<p>An adventure game in which players explore the ocean. One feature I like allows the player to take a picture and store it in an album. The game recognizes which fish are in the picture so it can reward them for collecting them all.</p>

<p><img src="platform.jpg" alt="platform screenshot" /></p>

<p>An action platform game where players avoid obstacles by jumping and running up walls.</p>

<p><img src="restock.jpg" alt="restock screenshot" /></p>

<p>A puzzle game I created for a school competition. Written in C++ and OpenGL.</p>

<p><img src="shooter.jpg" alt="shooter screenshot" /></p>

<p>A platform shooter game.</p>

<p><img src="strategy.jpg" alt="strategy screenshot" /></p>

<p>A prototype of a strategy game for iOS.</p>

 ]]></description>
</item>
<item>
<pubDate>Tue, 20 Aug 2013 00:00:00 </pubDate>
<title>C Craft</title>
<guid isPermaLink="true">https://github.com/justinmeiners/c-craft</guid>
</item>
<item>
<pubDate>Sat, 20 Jul 2013 00:00:00 </pubDate>
<title>IOCC Death Star</title>
<guid isPermaLink="true">https://github.com/justinmeiners/ioccc-death-star</guid>
</item>
<item>
<pubDate>Tue, 02 Jul 2013 00:00:00 </pubDate>
<title>Image Sequence Streaming</title>
<guid isPermaLink="true">https://github.com/justinmeiners/image-sequence-streaming</guid>
</item>
<item>
<pubDate>Wed, 22 May 2013 00:00:00 </pubDate>
<title>Mode 7</title>
<guid isPermaLink="true">https://github.com/justinmeiners/mode-7</guid>
</item>
<item>
<pubDate>Sun, 07 Apr 2013 00:00:00 </pubDate>
<title>C Foundation</title>
<guid isPermaLink="true">https://github.com/justinmeiners/c-foundation</guid>
</item>
<item>
<pubDate>Tue, 08 Jan 2013 00:00:00 </pubDate>
<title>iOS Color Wheel</title>
<guid isPermaLink="true">https://github.com/justinmeiners/ios-color-wheel</guid>
</item>
</channel></rss>