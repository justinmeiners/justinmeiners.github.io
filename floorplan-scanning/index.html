<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<title>Building Scanning Patents</title>
<link rel="stylesheet" href="/template/style.css">
<link rel="alternate" title="RSS feed" type="application/rss+xml" href="/feed.xml">
</head>
<body>
<p>
<a href="/">Home</a>
<a style="margin-left: 16px" href="https://github.com/justinmeiners">GitHub</a>
<a style="margin-left: 16px" href="/feed.xml">Rss</a>
</p>

<h1>Building Scanning Patents</h1>

<p>At a previous startup, I invented an Augmented Reality (AR) system for scanning buildings,
and automatically extracting floor plans from scans.
Recently a few of the patents I wrote were published, and I wanted to share those, with a brief summary.</p>

<h2>Capturing Environmental Features Using 2D and 3D Scans</h2>

<p><a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20230083703">US-20230083703-A1</a></p>

<p><img src="ar-scan.png" alt="ar scan" /></p>

<p>This describes an augmented reality workflow for capturing a 3D mesh of an environment, and recording points of interest.
A key idea is to capture all the user interactions and raw 3D data first.
Later in a post-process step, we determine user intent and relative positioning using complete global information.</p>

<h2>Floor Plan Extraction</h2>

<p><a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20240005570">US-20240005570-A1</a></p>

<p><img src="possible-scans.png" alt="possible-scans" /></p>

<p>Once we have a 3D scan of an environment, how do we go about turning this into a 2D floor plan?
Floor plans are conceptual, rather than objective, and so are guided by user input.
This invention describes both an augmented reality workflow and an algorithm for processing that input.
It turns out, doing this perfectly is likely NP-complete, but this proper understanding guides effective heuristics.</p>

<h2>Surface Animation During Dynamic Floor Plan Generation</h2>

<p><a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20240177385">US-20240177385-A1</a></p>

<p><img src="animation.png" alt="animation" /></p>

<p>It&rsquo;s important to give feedback to the user during the scanning process.
One way we do that is through the augmented reality visualization that shows which parts of a room are captured,
and is programmed through shaders.
The biggest challenge is to show accurate estimates quickly, despite the scan being incomplete.</p>

<h2>Aligning Polygon-like Representations With Inaccuracies</h2>

<p><a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20230334200">US-20230334200-A1</a></p>

<p><img src="align-shapes.png" alt="align shapes" /></p>

<p>Scan measurements contain error, and the real world is imperfect.
How can we correct the alignment of complex floor plan shapes with minimal distortion?
One answer is to simultaneously move all points of the floor plan in a <a href="/why-train-when-you-can-optimize/">multi-variable optimization</a> approach.</p>

<h2>Door and Window Detection in an AR Environment</h2>

<p><a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20240153096">US-20240153096-A1</a></p>

<p><img src="wall-3d.png" alt="wall 3D" /></p>

<p><img src="wall-2d.png" alt="wall 2D" /></p>

<p>Along with the shape of the building, we are also interested in the apertures of each room.
The polygonal shape tells us where the walls are in 3D.
We can then render the  portion of the 3D mesh corresponding to each wall, and detect cutouts in the rendered image.
This transforms a complex 3D detection problem into a traditional 2D computer vision problem.
These 2D detections are then projected back into 3D features on the wall.</p>
</body>
</html>
